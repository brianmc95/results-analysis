{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from itertools import repeat\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted\n",
    "import shutil\n",
    "\n",
    "import tempfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vectors(json_fields, single=False):\n",
    "    # Simple function to remove the vector from results in json file might be we remove that part from the json file\n",
    "    if single:\n",
    "        return json_fields.replace(\":vector\", \"\")\n",
    "    for i in range(len(json_fields)):\n",
    "        json_fields[i] = json_fields[i].replace(\":vector\", \"\")\n",
    "    return json_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_subprocess_output(pipe):\n",
    "    for line in iter(pipe.readline, b''):  # b'\\n'-separated lines\n",
    "        print('Subprocess Line: %r', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(lower_bound, width, quantity):\n",
    "    \"\"\" create_bins returns an equal-width (distance) partitioning.\n",
    "        It returns an ascending list of tuples, representing the intervals.\n",
    "        A tuple bins[i], i.e. (bins[i][0], bins[i][1])  with i > 0\n",
    "        and i < quantity, satisfies the following conditions:\n",
    "            (1) bins[i][0] + width == bins[i][1]\n",
    "            (2) bins[i-1][0] + width == bins[i][0] and\n",
    "                bins[i-1][1] + width == bins[i][1]\n",
    "    \"\"\"\n",
    "    bins = []\n",
    "    for low in range(lower_bound, lower_bound + quantity * width + 1, width):\n",
    "        bins.append((low, low + width))\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_data(temp_file, real_vector_path, json_fields, output_csv):\n",
    "    temp_file_pt = open(temp_file, \"w+\")\n",
    "\n",
    "    # Simply remove the :vector part of vector names from both sets of vectors.\n",
    "    found_vector = False\n",
    "    for field in json_fields:\n",
    "        if \":vector\" in field:\n",
    "            found_vector = True\n",
    "            break\n",
    "\n",
    "    if found_vector:\n",
    "        json_fields = remove_vectors(json_fields)\n",
    "\n",
    "    print(\"Beginning parsing of vector file: {}\".format(real_vector_path))\n",
    "\n",
    "    # Read the file and retrieve the list of vectors\n",
    "    read_vector_file(temp_file_pt, real_vector_path, json_fields)\n",
    "\n",
    "    print(\"Finished parsing of vector file: {}\".format(real_vector_path))\n",
    "\n",
    "    # Ensure we are at the start of the file for sorting\n",
    "    temp_file_pt.seek(0)\n",
    "\n",
    "    results = []\n",
    "    orphans = pd.DataFrame()\n",
    "    \n",
    "    # Tell pandas to read the data in chunks\n",
    "    chunks = pd.read_csv(temp_file_pt, chunksize=1e6)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "\n",
    "        # Add the previous orphans to the chunk\n",
    "        chunk = pd.concat((orphans, chunk))\n",
    "        \n",
    "        # Determine which rows are orphans\n",
    "        last_val = chunk[\"NodeID\"].iloc[-1]\n",
    "        is_orphan = chunk[\"NodeID\"] == last_val\n",
    "        \n",
    "        # Put the new orphans aside\n",
    "        chunk, orphans = chunk[~is_orphan], chunk[is_orphan]\n",
    "        \n",
    "        # Parse the vector file to ensure it is formatted correclty.\n",
    "        chunk['seq'] = chunk.groupby([\"EventNumber\", \"StatisticName\"]).cumcount()\n",
    "        chunk = chunk.pivot_table(\"Value\", [\"EventNumber\", \"Time\", \"NodeID\", \"seq\"], \"StatisticName\")\n",
    "        chunk.reset_index(inplace=True)\n",
    "        chunk = chunk.drop([\"seq\"], axis=1)\n",
    "        \n",
    "        results.append(chunk)\n",
    "        \n",
    "    print(\"Finsihed parsing output file: {}\".format(output_csv))\n",
    "\n",
    "    # Remove our temporary file.\n",
    "    os.remove(temp_file_pt.name)\n",
    "    print(\"Removed the temporary file\")\n",
    "    \n",
    "    resulting_df = pd.concat(results, sort=False, axis=0, ignore_index=True)\n",
    "    \n",
    "    resulting_df.to_csv(output_csv)\n",
    "\n",
    "    return resulting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_fields(df, fields, bin_width=10, bin_quantity=49):\n",
    "    \"\"\"\n",
    "    Bins multiple dfs into a single dictionary that can be used as an average for multiple fields across multiple\n",
    "    runs\n",
    "    :param df: dataframe to bin\n",
    "    :param fields: fields to be binned.\n",
    "    :param bin_width: width of each bin\n",
    "    :param bin_quantity: total number of bins\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bins = create_bins(lower_bound=0, width=bin_width, quantity=bin_quantity)\n",
    "    distances = []\n",
    "    overall_fields = {}\n",
    "    for interval in bins:\n",
    "        upper_b = interval[1]\n",
    "        distances.append(upper_b)\n",
    "\n",
    "    for field in fields:\n",
    "        print(\"{} being binned\".format(field))\n",
    "        overall_fields[field] = []\n",
    "\n",
    "    overall_fields[\"distance\"] = distances\n",
    "\n",
    "    distance_col = config[\"results\"][\"distance\"]\n",
    "\n",
    "    for i in range(len(bins)):\n",
    "        lower_b = bins[i][0]\n",
    "        upper_b = bins[i][1]\n",
    "        fields_temp = df[(df[distance_col] >= lower_b) & (df[distance_col] < upper_b)]\n",
    "        for field in fields:\n",
    "            if i < len(overall_fields[field]):\n",
    "                overall_fields[field][i] = (fields_temp[field].mean() + overall_fields[field][i]) / 2\n",
    "            else:\n",
    "                overall_fields[field].append(fields_temp[field].mean())\n",
    "\n",
    "    return overall_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(combined, results):\n",
    "    for result in results:\n",
    "        for field in result:\n",
    "            if field in combined:\n",
    "                for i in range(len(result[field])):\n",
    "                    combined[field][i] = (combined[field][i] + result[field][i]) / 2\n",
    "            else:\n",
    "                combined[field] = result[field]\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(results_dirs, now):\n",
    "    combined_results = {}\n",
    "\n",
    "    configs = []\n",
    "    for config in config[\"config_names\"]:\n",
    "        config_data = config[\"config_names\"][config]\n",
    "        if config_data[\"repeat\"] != 0:\n",
    "            if \"naming\" in config_data and len(config_data[\"naming\"]) > 0:\n",
    "                for name in config_data[\"naming\"]:\n",
    "                    configs.append(name)\n",
    "            else:\n",
    "                configs.append(config)\n",
    "\n",
    "    for result_dir, config_name in zip(results_dirs, configs):\n",
    "\n",
    "        folder_name = os.path.basename(result_dir)\n",
    "\n",
    "        print(\"Dealing with config: {} of result folder: {}\".format(config_name, folder_name))\n",
    "        combined_results[config_name] = {}\n",
    "\n",
    "        orig_loc = os.getcwd()\n",
    "\n",
    "        print(\"Moving to results dir: {}\".format(result_dir))\n",
    "        os.chdir(result_dir)\n",
    "\n",
    "        runs = []\n",
    "\n",
    "        for run in os.listdir(result_dir):\n",
    "            if \".vec\" in run:\n",
    "                runs.append(run)\n",
    "\n",
    "        num_processes = config[\"parallel_processes\"]\n",
    "        if num_processes > multiprocessing.cpu_count():\n",
    "            (\"Too many processes, going to revert to total - 1\")\n",
    "            num_processes = multiprocessing.cpu_count() - 1\n",
    "\n",
    "        print(\"Number of files to parse : {}\".format(len(runs)))\n",
    "        number_of_batches = len(runs) // num_processes\n",
    "        if number_of_batches == 0:\n",
    "            number_of_batches = 1\n",
    "\n",
    "        i = 0\n",
    "        while i < len(runs):\n",
    "            if len(runs) < num_processes:\n",
    "                num_processes = len(runs)\n",
    "            print(\n",
    "                \"Starting up processes, batch {}/{}\".format((i // num_processes) + 1, number_of_batches))\n",
    "            pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "            multiple_results = pool.starmap(filter_data, zip(runs[i:i + num_processes], repeat(config_name), repeat(now), repeat(orig_loc)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            combined_results[config_name] = combine_results(combined_results[config_name], multiple_results)\n",
    "\n",
    "            print(\"Batch {}/{} complete\".format((i // num_processes) + 1, number_of_batches))\n",
    "\n",
    "            i += num_processes\n",
    "\n",
    "        print(\"Moving back to original location: {}\".format(orig_loc))\n",
    "        os.chdir(orig_loc)\n",
    "\n",
    "    processed_file = \"{}/data/processed_data/{}-{}.json\".format(os.getcwd(), experiment_type, now)\n",
    "    print(\"Writing processed data to {}\".format(processed_file))\n",
    "    with open(processed_file, \"w\") as json_output:\n",
    "        json.dump(combined_results, json_output)\n",
    "\n",
    "    return processed_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_data(raw_data_file, config_name, now, orig_loc):\n",
    "\n",
    "    run_num = raw_data_file.split(\".\")[0]\n",
    "\n",
    "    temp_file_name = run_num + \".csv\"\n",
    "\n",
    "    print(\"File being parsed: {}\".format(temp_file_name))\n",
    "\n",
    "    output_csv_dir = \"{}/data/raw_data/{}/{}\".format(orig_loc, experiment_type, config_name)\n",
    "\n",
    "    os.makedirs(output_csv_dir, exist_ok=True)\n",
    "\n",
    "    output_csv = \"{}/{}-{}.csv\".format(output_csv_dir, run_num, now)\n",
    "\n",
    "    print(\"Raw output file: {}\".format(output_csv))\n",
    "\n",
    "    vector_df = tidy_data(temp_file_name, raw_data_file, results[\"filtered_vectors\"], output_csv)\n",
    "\n",
    "    print(\"Completed tidying of dataframes\")\n",
    "    \n",
    "    return\n",
    "\n",
    "    graphs = config[\"results\"][\"graphs\"]\n",
    "    print(\"The data for the following graphs must be prepared {}\".format(graphs))\n",
    "\n",
    "    if \":vector\" in config[\"results\"][\"decoded\"]:\n",
    "        # Assuming if decoded contains :vector then fails will too.\n",
    "        config[\"results\"][\"decoded\"]  = remove_vectors(config[\"results\"][\"decoded\"], single=True)\n",
    "        config[\"results\"][\"distance\"] = remove_vectors(config[\"results\"][\"distance\"], single=True)\n",
    "        config[\"results\"][\"fails\"]    = remove_vectors(config[\"results\"][\"fails\"])\n",
    "\n",
    "    fields = []\n",
    "    if \"pdr-dist\" in graphs:\n",
    "        print(\"Calculating pdr for pdr graph\")\n",
    "        fields.append(results[\"decoded\"])\n",
    "    if \"error-dist\" in graphs:\n",
    "        for fail in config[\"results\"][\"fails\"]:\n",
    "            fields.append(fail)\n",
    "        fields.append(results[\"decoded\"])\n",
    "\n",
    "    print(\"Binning all the necessary information for the graphs\")\n",
    "    binned_results = bin_fields(vector_df, fields)\n",
    "\n",
    "    del vector_df\n",
    "\n",
    "    print(\"Completed data parsing for this run\")\n",
    "    return binned_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New parsing system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vector_desc_line(line):\n",
    "    try:\n",
    "        # Converts a vector description line to a dictionary for use in parsing later\n",
    "        node_id_pattern = re.compile(\"\\[\\d+\\]\")\n",
    "\n",
    "        vector_line_dict = {\"nodeID\": None, \"vectorName\": None, \"ETV\": True}\n",
    "        split_line = line.split(\" \")\n",
    "        vector_num = int(split_line[1])\n",
    "        match = node_id_pattern.search(split_line[2])\n",
    "        nodeID = int(match.group().strip(\"[]\"))\n",
    "        vector_name = split_line[3]\n",
    "        vector_name = vector_name.split(\":\")[0]\n",
    "        if \"ETV\" in split_line[4]:\n",
    "            ETV = True\n",
    "        else:\n",
    "            ETV = False\n",
    "\n",
    "        vector_line_dict[\"nodeID\"] = nodeID\n",
    "        vector_line_dict[\"vectorName\"] = vector_name\n",
    "        vector_line_dict[\"ETV\"] = ETV\n",
    "\n",
    "        return vector_num, vector_line_dict\n",
    "    except AttributeError as e:\n",
    "        print(\"Line: {} : Could not be parsed\".format(line))\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vector_line(line):\n",
    "    # Simple function to split a vector line and convert to floats.\n",
    "    try:\n",
    "        line = bytes(line, 'utf-8').decode('utf-8', 'ignore')\n",
    "        split_nums = line.split()\n",
    "        for i in range(len(split_nums)):\n",
    "            split_nums[i] = float(split_nums[i])\n",
    "        return split_nums\n",
    "    except ValueError as e:\n",
    "        print(\"Line: {} could not be converted due to bad encoding\".format(line))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_csv_line(vector_dict, vector_id, parsed_vec):\n",
    "    # Parses the vector line information to be written to the csv file.\n",
    "    node_id = vector_dict[vector_id][\"nodeID\"]\n",
    "    vector_name = vector_dict[vector_id][\"vectorName\"]\n",
    "    if vector_dict[vector_id][\"ETV\"]:\n",
    "        etv = parsed_vec[1]\n",
    "        time = parsed_vec[2]\n",
    "        value = parsed_vec[3]\n",
    "    else:\n",
    "        etv = None\n",
    "        time = parsed_vec[1]\n",
    "        value = parsed_vec[2]\n",
    "\n",
    "    csv_line = [node_id, time, vector_name, value]\n",
    "    return csv_line, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chunk_writer(output_file, chunk_num, title_line):\n",
    "    # Setup our chunk writer\n",
    "    \n",
    "    # First create a folder to hold chunks\n",
    "    chunk_folder = output_file.split(\".\")[0]\n",
    "    os.makedirs(chunk_folder, exist_ok=True)\n",
    "    chunk_name = \"{}/chunk-{}.csv\".format(chunk_folder, chunk_num)\n",
    "    \n",
    "    # Create the chunk file and create a csv writer which uses it\n",
    "    print(\"Setting up new chunk: {}\".format(chunk_name))\n",
    "    temp_file_pt = open(chunk_name, \"w+\")\n",
    "    output_writer = csv.writer(temp_file_pt, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    output_writer.writerow(title_line)\n",
    "    \n",
    "    return temp_file_pt, output_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vector_file(output_file, vector_path, stats, chunk_size=1e+8):\n",
    "    \"\"\"\n",
    "    chunk_size: the time between different files 1.5s as default\n",
    "    \"\"\"\n",
    "    # Reads the csv file, parses it and writes to a temp file for use later in generating a DF and CSV file.\n",
    "    vector_dict = {}\n",
    "    no_interest_vectors = {} # Probably don't need to remember one's we don't care for.\n",
    "\n",
    "    chunk_times = []\n",
    "    chunk_info = {}\n",
    "    \n",
    "    last_time = -1\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    # Patterns which identify vector declaration lines and result lines\n",
    "    vector_dec_line_pattern = re.compile(\"^vector\")\n",
    "    vector_res_line_pattern = re.compile(\"^\\d+\")\n",
    "\n",
    "    vector_file = open(vector_path, \"r\")\n",
    "\n",
    "    # Stores lines appearing before their declaration. Files are oddly formatted, this is purely safety ensuring we\n",
    "    # don't accidentally miss anything.\n",
    "    early_vectors = tempfile.NamedTemporaryFile(mode=\"r+\")\n",
    "\n",
    "    # Prepare and write out first line format NodeID, EventNumber, Time, Stat1, Stat2, Stat3, ...\n",
    "    title_line = [\"NodeID\", \"Time\", \"StatisticName\", \"Value\"]\n",
    "    \n",
    "    temp_file_pt, writer = setup_chunk_writer(output_file, current_chunk_index, title_line)\n",
    "    chunk_info[\"CurrentChunk\"] = {\"file\": temp_file_pt, \"writer\": writer}\n",
    "    \n",
    "    for line in vector_file:\n",
    "        if vector_dec_line_pattern.match(line):\n",
    "            # if line matches a vector declaration, parse the vector description\n",
    "            vector_num, vec_dict = parse_vector_desc_line(line)\n",
    "            if vector_num is None and vec_dict is None:\n",
    "                continue\n",
    "            if vec_dict[\"vectorName\"] in stats:\n",
    "                # Vector is of interest, add it to our overall dictionary and update it's index.\n",
    "                vector_dict[vector_num] = vec_dict\n",
    "            else:\n",
    "                # Mark this as a vector we don't care about.\n",
    "                no_interest_vectors[vector_num] = None\n",
    "\n",
    "        elif vector_res_line_pattern.match(line):\n",
    "            # {\"nodeID\": None, \"vectorName\": None, \"ETV\": True} This is what it looks like\n",
    "            parsed_vec = parse_vector_line(line)\n",
    "            # If the previous step fails then we can simply continue to the next line ignoring this line.\n",
    "            if parsed_vec is None:\n",
    "                continue\n",
    "            vector_id = parsed_vec[0]\n",
    "            if vector_id in vector_dict:\n",
    "                # Write out to a csv file correctly\n",
    "                csv_line, time = prepare_csv_line(vector_dict, vector_id, parsed_vec)\n",
    "\n",
    "                if time > last_time:\n",
    "                    chunk_info[\"CurrentChunk\"][\"writer\"].writerow(csv_line)\n",
    "                    if chunk_info[\"CurrentChunk\"][\"file\"].tell() >= chunk_size:\n",
    "                        print(\"Time ending this chunk:{}\".format(time))\n",
    "                        \n",
    "                        # This chunk is old and as such can be placed into the previous chunks\n",
    "                        chunk_info[time] = {\"file\": chunk_info[\"CurrentChunk\"][\"file\"], \"writer\": chunk_info[\"CurrentChunk\"][\"writer\"]}\n",
    "                        chunk_times.append(time)\n",
    "                        last_time = time\n",
    "                        current_chunk_index += 1\n",
    "                        \n",
    "                        # This file is at max size, create a new writer\n",
    "                        temp_file_pt, writer = setup_chunk_writer(output_file, current_chunk_index, title_line)\n",
    "                        # Update current chunk writer to point at this new one.\n",
    "                        chunk_info[\"CurrentChunk\"] = {\"file\": temp_file_pt, \"writer\": writer}\n",
    "                if time <= last_time:\n",
    "                    for chunk_time in chunk_times:\n",
    "                        if time < chunk_time:\n",
    "                            chunk_info[chunk_time][\"writer\"].writerow(csv_line)\n",
    "            else:\n",
    "                if vector_id not in no_interest_vectors:\n",
    "                    # Write the line out in case we found it before declaration. Only if it is of possible interest.\n",
    "                    early_vectors.write(line)\n",
    "\n",
    "    # Rewind the early vectors file so we can search it for missed vectors\n",
    "    early_vectors.seek(0)\n",
    "\n",
    "    for line in early_vectors:\n",
    "        print(\"We have early vectors\")\n",
    "        # Parse the line again.\n",
    "        parsed_vec = parse_vector_line(line)\n",
    "        vector_id = parsed_vec[0]\n",
    "        # check for the vector\n",
    "        if vector_id in vector_dict:\n",
    "            # If we have it create the csv line and write it our\n",
    "            csv_line = prepare_csv_line(vector_dict, vector_id, parsed_vec)\n",
    "            output_writer.writerow(csv_line)\n",
    "\n",
    "    # Close our vector file.\n",
    "    vector_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_pivot(directory):\n",
    "    orig_loc = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "    \n",
    "    csv_files = os.listdir()\n",
    "    csv_files = natsorted(csv_files)\n",
    "    header = True\n",
    "    for csv_file in csv_files:\n",
    "        if \".csv\" in csv_file:\n",
    "            print(\"Dealing with chunk file: {}\".format(csv_file))\n",
    "            chunk_df = pd.read_csv(csv_file)\n",
    "\n",
    "            chunk_df = chunk_df.infer_objects()\n",
    "\n",
    "            chunk_df = chunk_df.sort_values(by=[\"NodeID\", \"Time\"])\n",
    "            # Parse the vector file to ensure it is formatted correclty.\n",
    "            chunk_df['seq'] = chunk_df.groupby([\"Time\", \"NodeID\", \"StatisticName\"]).cumcount()\n",
    "\n",
    "            chunk_df = chunk_df.pivot_table(\"Value\", [\"Time\", \"NodeID\", \"seq\"], \"StatisticName\")\n",
    "            chunk_df.reset_index(inplace=True)\n",
    "            chunk_df = chunk_df.drop([\"seq\"], axis=1)\n",
    "            chunk_df.to_csv(csv_file, index=False, header=header)\n",
    "            header = False\n",
    "\n",
    "            del chunk_df\n",
    "            \n",
    "    os.chdir(orig_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(csv_directory, outfile):\n",
    "    destination = open(outfile,'wb')\n",
    "    \n",
    "    orig_loc = os.getcwd()\n",
    "    os.chdir(csv_directory)\n",
    "    \n",
    "    csv_files = os.listdir()\n",
    "    csv_files = natsorted(csv_files)\n",
    "    header = True\n",
    "    for csv_file in csv_files:\n",
    "        if \".csv\" in csv_file and csv_file != outfile:\n",
    "            print(\"Dealing with chunk file: {}\".format(csv_file))\n",
    "            shutil.copyfileobj(open(csv_file,'rb'), destination)\n",
    "            os.remove(csv_file)\n",
    "    destination.close()\n",
    "    \n",
    "    os.chdir(orig_loc)\n",
    "    \n",
    "    os.rmdir(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dir = \"/Users/brianmccarthy/git_repos/results-analysis/data/omnet/cv2x/test\"\n",
    "\n",
    "vector_file_name_long = \"long.vec\"\n",
    "vector_file_name_short = \"short.vec\"\n",
    "\n",
    "vector_path_long = vector_dir + vector_file_name_long\n",
    "vector_path_short = vector_dir + vector_file_name_short\n",
    "\n",
    "config_name_long = \"long-test\"\n",
    "config_name_short = \"short-test\"\n",
    "\n",
    "experiment_type = \"cv2x\"\n",
    "\n",
    "json_path = \"/Users/brianmccarthy/git_repos/results-analysis/configs/cv2x.json\"\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "orig_loc = \"/Users/brianmccarthy/git_repos/results-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, \"r\") as json_file:\n",
    "    config = json.load(json_file)[\"cv2x\"]\n",
    "    results = config[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = \"long.csv\"\n",
    "real_vector_path = \"long.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for dealing with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_fields = results[\"filtered_vectors\"]\n",
    "# Simply remove the :vector part of vector names from both sets of vectors.\n",
    "found_vector = False\n",
    "for field in json_fields:\n",
    "    if \":vector\" in field:\n",
    "        found_vector = True\n",
    "        break\n",
    "\n",
    "if found_vector:\n",
    "    json_fields = remove_vectors(json_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up new chunk: long/chunk-0.csv\n",
      "Time ending this chunk:500.824\n",
      "Setting up new chunk: long/chunk-1.csv\n",
      "Time ending this chunk:501.227\n",
      "Setting up new chunk: long/chunk-2.csv\n",
      "Time ending this chunk:501.574\n",
      "Setting up new chunk: long/chunk-3.csv\n",
      "Time ending this chunk:501.885\n",
      "Setting up new chunk: long/chunk-4.csv\n",
      "Time ending this chunk:502.225\n",
      "Setting up new chunk: long/chunk-5.csv\n",
      "Time ending this chunk:502.544\n",
      "Setting up new chunk: long/chunk-6.csv\n",
      "Time ending this chunk:502.866\n",
      "Setting up new chunk: long/chunk-7.csv\n",
      "Time ending this chunk:503.215\n",
      "Setting up new chunk: long/chunk-8.csv\n",
      "Time ending this chunk:503.577\n",
      "Setting up new chunk: long/chunk-9.csv\n",
      "Time ending this chunk:503.92\n",
      "Setting up new chunk: long/chunk-10.csv\n",
      "Time ending this chunk:504.275\n",
      "Setting up new chunk: long/chunk-11.csv\n",
      "Time ending this chunk:504.586\n",
      "Setting up new chunk: long/chunk-12.csv\n",
      "Time ending this chunk:504.962\n",
      "Setting up new chunk: long/chunk-13.csv\n",
      "Time ending this chunk:505.3\n",
      "Setting up new chunk: long/chunk-14.csv\n",
      "Time ending this chunk:505.613\n",
      "Setting up new chunk: long/chunk-15.csv\n",
      "Time ending this chunk:505.974\n",
      "Setting up new chunk: long/chunk-16.csv\n",
      "Time ending this chunk:506.297\n",
      "Setting up new chunk: long/chunk-17.csv\n",
      "Time ending this chunk:506.632\n",
      "Setting up new chunk: long/chunk-18.csv\n",
      "Time ending this chunk:506.957\n",
      "Setting up new chunk: long/chunk-19.csv\n",
      "Time ending this chunk:507.282\n",
      "Setting up new chunk: long/chunk-20.csv\n",
      "Time ending this chunk:507.63\n",
      "Setting up new chunk: long/chunk-21.csv\n",
      "Time ending this chunk:507.964\n",
      "Setting up new chunk: long/chunk-22.csv\n",
      "Time ending this chunk:508.31\n",
      "Setting up new chunk: long/chunk-23.csv\n",
      "Time ending this chunk:508.663\n",
      "Setting up new chunk: long/chunk-24.csv\n",
      "Time ending this chunk:509.006\n",
      "Setting up new chunk: long/chunk-25.csv\n",
      "Time ending this chunk:509.362\n",
      "Setting up new chunk: long/chunk-26.csv\n",
      "Time ending this chunk:509.68\n",
      "Setting up new chunk: long/chunk-27.csv\n",
      "Time ending this chunk:510.021\n",
      "Setting up new chunk: long/chunk-28.csv\n",
      "Time ending this chunk:510.38\n",
      "Setting up new chunk: long/chunk-29.csv\n",
      "Time ending this chunk:510.717\n",
      "Setting up new chunk: long/chunk-30.csv\n",
      "Time ending this chunk:511.038\n",
      "Setting up new chunk: long/chunk-31.csv\n",
      "Time ending this chunk:511.398\n",
      "Setting up new chunk: long/chunk-32.csv\n",
      "Time ending this chunk:511.738\n",
      "Setting up new chunk: long/chunk-33.csv\n",
      "Reading Execution took 1346.8332810401917s\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'short'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-27bea5d0a86e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading Execution took {}s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcsv_pivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"short\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pivoting Execution took {}s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-57a2767e731f>\u001b[0m in \u001b[0;36mcsv_pivot\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcsv_pivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0morig_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcsv_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'short'"
     ]
    }
   ],
   "source": [
    "os.chdir(vector_dir)\n",
    "\n",
    "# Read in all the vectors\n",
    "start_time = time.time()\n",
    "overall_start_time = start_time\n",
    "\n",
    "read_vector_file(temp_file, real_vector_path, json_fields)\n",
    "print(\"Reading Execution took {}s\".format(time.time() - start_time))\n",
    "start_time = time.time()\n",
    "csv_pivot()\n",
    "print(\"Pivoting Execution took {}s\".format(time.time() - start_time))\n",
    "start_time = time.time()\n",
    "combine_files(, )\n",
    "print(\"Copying Execution took {}s\".format(time.time() - start_time))\n",
    "\n",
    "print(\"Overall Execution took {}s\".format(time.time() - overall_start_time))\n",
    "\n",
    "os.chdir(orig_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing parsed result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/long-test/longRun-1-2019-09-19-16:17:32.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
