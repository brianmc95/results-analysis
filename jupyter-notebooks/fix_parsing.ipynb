{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from itertools import repeat\n",
    "import datetime\n",
    "\n",
    "import tempfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vectors(json_fields, single=False):\n",
    "    # Simple function to remove the vector from results in json file might be we remove that part from the json file\n",
    "    if single:\n",
    "        return json_fields.replace(\":vector\", \"\")\n",
    "    for i in range(len(json_fields)):\n",
    "        json_fields[i] = json_fields[i].replace(\":vector\", \"\")\n",
    "    return json_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_subprocess_output(pipe):\n",
    "    for line in iter(pipe.readline, b''):  # b'\\n'-separated lines\n",
    "        print('Subprocess Line: %r', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(lower_bound, width, quantity):\n",
    "    \"\"\" create_bins returns an equal-width (distance) partitioning.\n",
    "        It returns an ascending list of tuples, representing the intervals.\n",
    "        A tuple bins[i], i.e. (bins[i][0], bins[i][1])  with i > 0\n",
    "        and i < quantity, satisfies the following conditions:\n",
    "            (1) bins[i][0] + width == bins[i][1]\n",
    "            (2) bins[i-1][0] + width == bins[i][0] and\n",
    "                bins[i-1][1] + width == bins[i][1]\n",
    "    \"\"\"\n",
    "    bins = []\n",
    "    for low in range(lower_bound, lower_bound + quantity * width + 1, width):\n",
    "        bins.append((low, low + width))\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vector_desc_line(line):\n",
    "    try:\n",
    "        # Converts a vector description line to a dictionary for use in parsing later\n",
    "        node_id_pattern = re.compile(\"\\[\\d+\\]\")\n",
    "\n",
    "        vector_line_dict = {\"nodeID\": None, \"vectorName\": None, \"ETV\": True}\n",
    "        split_line = line.split(\" \")\n",
    "        vector_num = int(split_line[1])\n",
    "        match = node_id_pattern.search(split_line[2])\n",
    "        nodeID = int(match.group().strip(\"[]\"))\n",
    "        vector_name = split_line[3]\n",
    "        vector_name = vector_name.split(\":\")[0]\n",
    "        if \"ETV\" in split_line[4]:\n",
    "            ETV = True\n",
    "        else:\n",
    "            ETV = False\n",
    "\n",
    "        vector_line_dict[\"nodeID\"] = nodeID\n",
    "        vector_line_dict[\"vectorName\"] = vector_name\n",
    "        vector_line_dict[\"ETV\"] = ETV\n",
    "\n",
    "        return vector_num, vector_line_dict\n",
    "    except AttributeError as e:\n",
    "        print(\"Line: {} : Could not be parsed\".format(line))\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vector_line(line):\n",
    "    # Simple function to split a vector line and convert to floats.\n",
    "    try:\n",
    "        line = bytes(line, 'utf-8').decode('utf-8', 'ignore')\n",
    "        split_nums = line.split()\n",
    "        for i in range(len(split_nums)):\n",
    "            split_nums[i] = float(split_nums[i])\n",
    "        return split_nums\n",
    "    except ValueError as e:\n",
    "        print(\"Line: {} could not be converted due to bad encoding\".format(line))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_csv_line(vector_dict, vector_id, parsed_vec):\n",
    "    # Parses the vector line information to be written to the csv file.\n",
    "    node_id = vector_dict[vector_id][\"nodeID\"]\n",
    "    vector_name = vector_dict[vector_id][\"vectorName\"]\n",
    "    if vector_dict[vector_id][\"ETV\"]:\n",
    "        etv = parsed_vec[1]\n",
    "        time = parsed_vec[2]\n",
    "        value = parsed_vec[3]\n",
    "    else:\n",
    "        etv = None\n",
    "        time = parsed_vec[1]\n",
    "        value = parsed_vec[2]\n",
    "\n",
    "    csv_line = [node_id, etv, time, vector_name, value]\n",
    "    return csv_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vector_file(output_file, vector_path, stats):\n",
    "    # Reads the csv file, parses it and writes to a temp file for use later in generating a DF and CSV file.\n",
    "    vector_dict = {}\n",
    "    no_interest_vectors = {}\n",
    "\n",
    "    # Patterns which identify vector declaration lines and result lines\n",
    "    vector_dec_line_pattern = re.compile(\"^vector\")\n",
    "    vector_res_line_pattern = re.compile(\"^\\d+\")\n",
    "\n",
    "    vector_file = open(vector_path, \"r\")\n",
    "\n",
    "    # Stores lines appearing before their declaration. Files are oddly formatted, this is purely safety ensuring we\n",
    "    # don't accidentally miss anything.\n",
    "    early_vectors = tempfile.NamedTemporaryFile(mode=\"r+\")\n",
    "\n",
    "    output_writer = csv.writer(output_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    # Prepare and write out first line format NodeID, EventNumber, Time, Stat1, Stat2, Stat3, ...\n",
    "    title_line = [\"NodeID\", \"EventNumber\", \"Time\", \"StatisticName\", \"Value\"]\n",
    "\n",
    "    output_writer.writerow(title_line)\n",
    "\n",
    "    for line in vector_file:\n",
    "        if vector_dec_line_pattern.match(line):\n",
    "            # if line matches a vector declaration, parse the vector description\n",
    "            vector_num, vec_dict = parse_vector_desc_line(line)\n",
    "            if vector_num is None and vec_dict is None:\n",
    "                continue\n",
    "            if vec_dict[\"vectorName\"] in stats:\n",
    "                # Vector is of interest, add it to our overall dictionary and update it's index.\n",
    "                vector_dict[vector_num] = vec_dict\n",
    "            else:\n",
    "                # Mark this as a vector we don't care about.\n",
    "                no_interest_vectors[vector_num] = None\n",
    "\n",
    "        elif vector_res_line_pattern.match(line):\n",
    "            parsed_vec = parse_vector_line(line)\n",
    "            # If the previous step fails then we can simply continue to the next line ignoring this line.\n",
    "            if parsed_vec is None:\n",
    "                continue\n",
    "            vector_id = parsed_vec[0]\n",
    "            if vector_id in vector_dict:\n",
    "                # Write out to a csv file correctly\n",
    "                csv_line = prepare_csv_line(vector_dict, vector_id, parsed_vec)\n",
    "                output_writer.writerow(csv_line)\n",
    "            else:\n",
    "                if vector_id not in no_interest_vectors:\n",
    "                    # Write the line out in case we found it before declaration. Only if it is of possible interest.\n",
    "                    early_vectors.write(line)\n",
    "\n",
    "    # Rewind the early vectors file so we can search it for missed vectors\n",
    "    early_vectors.seek(0)\n",
    "\n",
    "    for line in early_vectors:\n",
    "        # Parse the line again.\n",
    "        parsed_vec = parse_vector_line(line)\n",
    "        vector_id = parsed_vec[0]\n",
    "        # check for the vector\n",
    "        if vector_id in vector_dict:\n",
    "            # If we have it create the csv line and write it our\n",
    "            csv_line = prepare_csv_line(vector_dict, vector_id, parsed_vec)\n",
    "            output_writer.writerow(csv_line)\n",
    "\n",
    "    # Close our vector file.\n",
    "    vector_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_fields(df, fields, bin_width=10, bin_quantity=49):\n",
    "    \"\"\"\n",
    "    Bins multiple dfs into a single dictionary that can be used as an average for multiple fields across multiple\n",
    "    runs\n",
    "    :param df: dataframe to bin\n",
    "    :param fields: fields to be binned.\n",
    "    :param bin_width: width of each bin\n",
    "    :param bin_quantity: total number of bins\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bins = create_bins(lower_bound=0, width=bin_width, quantity=bin_quantity)\n",
    "    distances = []\n",
    "    overall_fields = {}\n",
    "    for interval in bins:\n",
    "        upper_b = interval[1]\n",
    "        distances.append(upper_b)\n",
    "\n",
    "    for field in fields:\n",
    "        print(\"{} being binned\".format(field))\n",
    "        overall_fields[field] = []\n",
    "\n",
    "    overall_fields[\"distance\"] = distances\n",
    "\n",
    "    distance_col = config[\"results\"][\"distance\"]\n",
    "\n",
    "    for i in range(len(bins)):\n",
    "        lower_b = bins[i][0]\n",
    "        upper_b = bins[i][1]\n",
    "        fields_temp = df[(df[distance_col] >= lower_b) & (df[distance_col] < upper_b)]\n",
    "        for field in fields:\n",
    "            if i < len(overall_fields[field]):\n",
    "                overall_fields[field][i] = (fields_temp[field].mean() + overall_fields[field][i]) / 2\n",
    "            else:\n",
    "                overall_fields[field].append(fields_temp[field].mean())\n",
    "\n",
    "    return overall_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(combined, results):\n",
    "    for result in results:\n",
    "        for field in result:\n",
    "            if field in combined:\n",
    "                for i in range(len(result[field])):\n",
    "                    combined[field][i] = (combined[field][i] + result[field][i]) / 2\n",
    "            else:\n",
    "                combined[field] = result[field]\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(results_dirs, now):\n",
    "    combined_results = {}\n",
    "\n",
    "    configs = []\n",
    "    for config in config[\"config_names\"]:\n",
    "        config_data = config[\"config_names\"][config]\n",
    "        if config_data[\"repeat\"] != 0:\n",
    "            if \"naming\" in config_data and len(config_data[\"naming\"]) > 0:\n",
    "                for name in config_data[\"naming\"]:\n",
    "                    configs.append(name)\n",
    "            else:\n",
    "                configs.append(config)\n",
    "\n",
    "    for result_dir, config_name in zip(results_dirs, configs):\n",
    "\n",
    "        folder_name = os.path.basename(result_dir)\n",
    "\n",
    "        print(\"Dealing with config: {} of result folder: {}\".format(config_name, folder_name))\n",
    "        combined_results[config_name] = {}\n",
    "\n",
    "        orig_loc = os.getcwd()\n",
    "\n",
    "        print(\"Moving to results dir: {}\".format(result_dir))\n",
    "        os.chdir(result_dir)\n",
    "\n",
    "        runs = []\n",
    "\n",
    "        for run in os.listdir(result_dir):\n",
    "            if \".vec\" in run:\n",
    "                runs.append(run)\n",
    "\n",
    "        num_processes = config[\"parallel_processes\"]\n",
    "        if num_processes > multiprocessing.cpu_count():\n",
    "            (\"Too many processes, going to revert to total - 1\")\n",
    "            num_processes = multiprocessing.cpu_count() - 1\n",
    "\n",
    "        print(\"Number of files to parse : {}\".format(len(runs)))\n",
    "        number_of_batches = len(runs) // num_processes\n",
    "        if number_of_batches == 0:\n",
    "            number_of_batches = 1\n",
    "\n",
    "        i = 0\n",
    "        while i < len(runs):\n",
    "            if len(runs) < num_processes:\n",
    "                num_processes = len(runs)\n",
    "            print(\n",
    "                \"Starting up processes, batch {}/{}\".format((i // num_processes) + 1, number_of_batches))\n",
    "            pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "            multiple_results = pool.starmap(filter_data, zip(runs[i:i + num_processes], repeat(config_name), repeat(now), repeat(orig_loc)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            combined_results[config_name] = combine_results(combined_results[config_name], multiple_results)\n",
    "\n",
    "            print(\"Batch {}/{} complete\".format((i // num_processes) + 1, number_of_batches))\n",
    "\n",
    "            i += num_processes\n",
    "\n",
    "        print(\"Moving back to original location: {}\".format(orig_loc))\n",
    "        os.chdir(orig_loc)\n",
    "\n",
    "    processed_file = \"{}/data/processed_data/{}-{}.json\".format(os.getcwd(), experiment_type, now)\n",
    "    print(\"Writing processed data to {}\".format(processed_file))\n",
    "    with open(processed_file, \"w\") as json_output:\n",
    "        json.dump(combined_results, json_output)\n",
    "\n",
    "    return processed_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(raw_data_file, config_name, now, orig_loc):\n",
    "\n",
    "    run_num = raw_data_file.split(\".\")[0]\n",
    "\n",
    "    temp_file_name = run_num + \".csv\"\n",
    "\n",
    "    print(\"File being parsed: {}\".format(temp_file_name))\n",
    "\n",
    "    output_csv_dir = \"{}/data/raw_data/{}/{}\".format(orig_loc, experiment_type, config_name)\n",
    "\n",
    "    os.makedirs(output_csv_dir, exist_ok=True)\n",
    "\n",
    "    output_csv = \"{}/{}-{}.csv\".format(output_csv_dir, run_num, now)\n",
    "\n",
    "    print(\"Raw output file: {}\".format(output_csv))\n",
    "\n",
    "    vector_df = tidy_data(temp_file_name, raw_data_file, results[\"filtered_vectors\"], output_csv)\n",
    "\n",
    "    print(\"Completed tidying of dataframes\")\n",
    "\n",
    "    graphs = config[\"results\"][\"graphs\"]\n",
    "    print(\"The data for the following graphs must be prepared {}\".format(graphs))\n",
    "\n",
    "    if \":vector\" in config[\"results\"][\"decoded\"]:\n",
    "        # Assuming if decoded contains :vector then fails will too.\n",
    "        config[\"results\"][\"decoded\"]  = remove_vectors(config[\"results\"][\"decoded\"], single=True)\n",
    "        config[\"results\"][\"distance\"] = remove_vectors(config[\"results\"][\"distance\"], single=True)\n",
    "        config[\"results\"][\"fails\"]    = remove_vectors(config[\"results\"][\"fails\"])\n",
    "\n",
    "    fields = []\n",
    "    if \"pdr-dist\" in graphs:\n",
    "        print(\"Calculating pdr for pdr graph\")\n",
    "        fields.append(results[\"decoded\"])\n",
    "    if \"error-dist\" in graphs:\n",
    "        for fail in config[\"results\"][\"fails\"]:\n",
    "            fields.append(fail)\n",
    "        fields.append(results[\"decoded\"])\n",
    "\n",
    "    print(\"Binning all the necessary information for the graphs\")\n",
    "    binned_results = bin_fields(vector_df, fields)\n",
    "\n",
    "    del vector_df\n",
    "\n",
    "    print(\"Completed data parsing for this run\")\n",
    "    return binned_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_data(temp_file, real_vector_path, json_fields, output_csv):\n",
    "    temp_file_pt = open(temp_file, \"w+\")\n",
    "\n",
    "    # Simply remove the :vector part of vector names from both sets of vectors.\n",
    "    found_vector = False\n",
    "    for field in json_fields:\n",
    "        if \":vector\" in field:\n",
    "            found_vector = True\n",
    "            break\n",
    "\n",
    "    if found_vector:\n",
    "        json_fields = remove_vectors(json_fields)\n",
    "\n",
    "    print(\"Beginning parsing of vector file: {}\".format(real_vector_path))\n",
    "\n",
    "    # Read the file and retrieve the list of vectors\n",
    "    read_vector_file(temp_file_pt, real_vector_path, json_fields)\n",
    "\n",
    "    print(\"Finished parsing of vector file: {}\".format(real_vector_path))\n",
    "\n",
    "    # Ensure we are at the start of the file for sorting\n",
    "    temp_file_pt.seek(0)\n",
    "\n",
    "    results = []\n",
    "    orphans = pd.DataFrame()\n",
    "    \n",
    "    # Tell pandas to read the data in chunks\n",
    "    chunks = pd.read_csv(temp_file_pt, chunksize=1e6)\n",
    "    \n",
    "    first_chunk = True\n",
    "    \n",
    "    for chunk in chunks:\n",
    "\n",
    "        # Add the previous orphans to the chunk\n",
    "        chunk = pd.concat((orphans, chunk))\n",
    "        \n",
    "        # Determine which rows are orphans\n",
    "        last_val = chunk[\"NodeID\"].iloc[-1]\n",
    "        is_orphan = chunk[\"NodeID\"] == last_val\n",
    "        \n",
    "        # Put the new orphans aside\n",
    "        chunk, orphans = chunk[~is_orphan], chunk[is_orphan]\n",
    "        \n",
    "        # Parse the vector file to ensure it is formatted correclty.\n",
    "        chunk['seq'] = chunk.groupby([\"EventNumber\", \"StatisticName\"]).cumcount()\n",
    "        chunk = chunk.pivot_table(\"Value\", [\"EventNumber\", \"Time\", \"NodeID\", \"seq\"], \"StatisticName\")\n",
    "        chunk.reset_index(inplace=True)\n",
    "        chunk = chunk.drop([\"seq\"], axis=1)\n",
    "        \n",
    "        chunk.to_csv(output_csv, mode=\"a\", index=False, header = first_chunk\n",
    "    )\n",
    "        \n",
    "    print(\"Finsihed parsing output file: {}\".format(output_csv))\n",
    "\n",
    "    # Remove our temporary file.\n",
    "    os.remove(temp_file_pt.name)\n",
    "    print(\"Removed the temporary file\")\n",
    "\n",
    "    return pd.read_csv(output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for dealing with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dir_long = \"/Users/brianmccarthy/git_repos/results-analysis/data/omnet/cv2x/long-test/\"\n",
    "vector_dir_short = \"/Users/brianmccarthy/git_repos/results-analysis/data/omnet/cv2x/short-test/\"\n",
    "\n",
    "vector_file_name_long = \"longRun-1.vec\"\n",
    "vector_file_name_short = \"shortRun-1.vec\"\n",
    "\n",
    "vector_path_long = vector_dir_long + vector_file_name_long\n",
    "vector_path_short = vector_dir_short + vector_file_name_short\n",
    "\n",
    "config_name_long = \"long-test\"\n",
    "config_name_short = \"short-test\"\n",
    "\n",
    "experiment_type = \"cv2x\"\n",
    "\n",
    "json_path = \"/Users/brianmccarthy/git_repos/results-analysis/configs/cv2x.json\"\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "orig_loc = \"/Users/brianmccarthy/git_repos/results-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, \"r\") as json_file:\n",
    "    config = json.load(json_file)[\"cv2x\"]\n",
    "    results = config[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File being parsed: longRun-1.csv\n",
      "Raw output file: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/long-test/longRun-1-2019-09-19-16:17:32.csv\n",
      "Beginning parsing of vector file: longRun-1.vec\n",
      "Finished parsing of vector file: longRun-1.vec\n",
      "Finsihed parsing output file: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/long-test/longRun-1-2019-09-19-16:17:32.csv\n",
      "Removed the temporary file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianmccarthy/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3296: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tidying of dataframes\n",
      "The data for the following graphs must be prepared ['pdr-dist', 'error-dist']\n",
      "Calculating pdr for pdr graph\n",
      "Binning all the necessary information for the graphs\n",
      "tbDecoded being binned\n",
      "tbFailedButSCIReceived being binned\n",
      "tbFailedDueToNoSCI being binned\n",
      "tbFailedHalfDuplex being binned\n",
      "tbDecoded being binned\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3b827476fd48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlong_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_dir_long\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfilter_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_file_name_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ec3e313f561a>\u001b[0m in \u001b[0;36mfilter_data\u001b[0;34m(raw_data_file, config_name, now, orig_loc)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Binning all the necessary information for the graphs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mbinned_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbin_fields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mvector_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a623b9bcf3a7>\u001b[0m in \u001b[0;36mbin_fields\u001b[0;34m(df, fields, bin_width, bin_quantity)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlower_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mupper_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfields_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdistance_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlower_b\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdistance_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mupper_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall_fields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m                 raise TypeError('Could not compare {typ} type with Series'\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_comp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_datetimelike_v_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_comp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/ops.pyx\u001b[0m in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "os.chdir(orig_loc)\n",
    "\n",
    "if long_run:\n",
    "    os.chdir(vector_dir_long)\n",
    "    filter_data(vector_file_name_long, config_name_long, now, orig_loc)\n",
    "    os.chdir(orig_loc)\n",
    "else:\n",
    "    os.chdir(vector_dir_short)\n",
    "    filter_data(vector_file_name_short, config_name_short, now, orig_loc)\n",
    "    os.chdir(orig_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing parsed result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/long-test/longRun-1-2019-09-19-16:17:32.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventNumber</th>\n",
       "      <th>Time</th>\n",
       "      <th>NodeID</th>\n",
       "      <th>cbr</th>\n",
       "      <th>sciDecoded</th>\n",
       "      <th>sciFailedHalfDuplex</th>\n",
       "      <th>sciNotDecoded</th>\n",
       "      <th>sciReceived</th>\n",
       "      <th>sciSent</th>\n",
       "      <th>tbDecoded</th>\n",
       "      <th>tbFailedButSCIReceived</th>\n",
       "      <th>tbFailedDueToNoSCI</th>\n",
       "      <th>tbFailedHalfDuplex</th>\n",
       "      <th>tbReceived</th>\n",
       "      <th>tbSent</th>\n",
       "      <th>txRxDistanceSCI</th>\n",
       "      <th>txRxDistanceTB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37145</td>\n",
       "      <td>400.106</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37146</td>\n",
       "      <td>400.106</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37148</td>\n",
       "      <td>400.106</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37149</td>\n",
       "      <td>400.106</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38198</td>\n",
       "      <td>400.107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1145.56</td>\n",
       "      <td>1145.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  EventNumber     Time NodeID  cbr sciDecoded sciFailedHalfDuplex  \\\n",
       "0       37145  400.106     54  NaN        NaN                 NaN   \n",
       "1       37146  400.106     54  NaN        NaN                 NaN   \n",
       "2       37148  400.106     69  NaN        NaN                 NaN   \n",
       "3       37149  400.106     69  NaN        NaN                 NaN   \n",
       "4       38198  400.107      0    0          0                   0   \n",
       "\n",
       "  sciNotDecoded sciReceived sciSent tbDecoded tbFailedButSCIReceived  \\\n",
       "0           NaN         NaN       1       NaN                    NaN   \n",
       "1           NaN         NaN     NaN       NaN                    NaN   \n",
       "2           NaN         NaN       1       NaN                    NaN   \n",
       "3           NaN         NaN     NaN       NaN                    NaN   \n",
       "4             1           1     NaN         0                      0   \n",
       "\n",
       "  tbFailedDueToNoSCI tbFailedHalfDuplex tbReceived tbSent txRxDistanceSCI  \\\n",
       "0                NaN                NaN        NaN    NaN             NaN   \n",
       "1                NaN                NaN        NaN      1             NaN   \n",
       "2                NaN                NaN        NaN    NaN             NaN   \n",
       "3                NaN                NaN        NaN      1             NaN   \n",
       "4                  1                  0          1    NaN         1145.56   \n",
       "\n",
       "  txRxDistanceTB  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4        1145.56  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventNumber</th>\n",
       "      <th>Time</th>\n",
       "      <th>NodeID</th>\n",
       "      <th>cbr</th>\n",
       "      <th>sciDecoded</th>\n",
       "      <th>sciFailedHalfDuplex</th>\n",
       "      <th>sciNotDecoded</th>\n",
       "      <th>sciReceived</th>\n",
       "      <th>sciSent</th>\n",
       "      <th>tbDecoded</th>\n",
       "      <th>tbFailedButSCIReceived</th>\n",
       "      <th>tbFailedDueToNoSCI</th>\n",
       "      <th>tbFailedHalfDuplex</th>\n",
       "      <th>tbReceived</th>\n",
       "      <th>tbSent</th>\n",
       "      <th>txRxDistanceSCI</th>\n",
       "      <th>txRxDistanceTB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3474211</td>\n",
       "      <td>3474211.000</td>\n",
       "      <td>3474211</td>\n",
       "      <td>1.617982e+06</td>\n",
       "      <td>3434465.0</td>\n",
       "      <td>3434465.0</td>\n",
       "      <td>3434465.0</td>\n",
       "      <td>3434465.0</td>\n",
       "      <td>19892.0</td>\n",
       "      <td>3434465.0</td>\n",
       "      <td>3434465.0</td>\n",
       "      <td>3434465.0</td>\n",
       "      <td>3434465.0</td>\n",
       "      <td>3434465.0</td>\n",
       "      <td>19892.0</td>\n",
       "      <td>3434465</td>\n",
       "      <td>3434465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1657702</td>\n",
       "      <td>17720.000</td>\n",
       "      <td>349</td>\n",
       "      <td>4.110000e+02</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>365548</td>\n",
       "      <td>365548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>EventNumber</td>\n",
       "      <td>410.812</td>\n",
       "      <td>88</td>\n",
       "      <td>4.931973e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>txRxDistanceSCI</td>\n",
       "      <td>txRxDistanceTB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>39</td>\n",
       "      <td>1892.000</td>\n",
       "      <td>13156</td>\n",
       "      <td>5.552700e+04</td>\n",
       "      <td>1556369.0</td>\n",
       "      <td>2152318.0</td>\n",
       "      <td>1535281.0</td>\n",
       "      <td>2152319.0</td>\n",
       "      <td>11424.0</td>\n",
       "      <td>1578886.0</td>\n",
       "      <td>2150888.0</td>\n",
       "      <td>1535282.0</td>\n",
       "      <td>2152317.0</td>\n",
       "      <td>2152319.0</td>\n",
       "      <td>11424.0</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EventNumber         Time   NodeID           cbr  sciDecoded  \\\n",
       "count       3474211  3474211.000  3474211  1.617982e+06   3434465.0   \n",
       "unique      1657702    17720.000      349  4.110000e+02         5.0   \n",
       "top     EventNumber      410.812       88  4.931973e-01         0.0   \n",
       "freq             39     1892.000    13156  5.552700e+04   1556369.0   \n",
       "\n",
       "        sciFailedHalfDuplex  sciNotDecoded  sciReceived  sciSent  tbDecoded  \\\n",
       "count             3434465.0      3434465.0    3434465.0  19892.0  3434465.0   \n",
       "unique                  5.0            5.0          5.0      3.0        5.0   \n",
       "top                     0.0            1.0          1.0      1.0        0.0   \n",
       "freq              2152318.0      1535281.0    2152319.0  11424.0  1578886.0   \n",
       "\n",
       "        tbFailedButSCIReceived  tbFailedDueToNoSCI  tbFailedHalfDuplex  \\\n",
       "count                3434465.0           3434465.0           3434465.0   \n",
       "unique                     5.0                 5.0                 5.0   \n",
       "top                        0.0                 1.0                 0.0   \n",
       "freq                 2150888.0           1535282.0           2152317.0   \n",
       "\n",
       "        tbReceived   tbSent  txRxDistanceSCI  txRxDistanceTB  \n",
       "count    3434465.0  19892.0          3434465         3434465  \n",
       "unique         5.0      3.0           365548          365548  \n",
       "top            1.0      1.0  txRxDistanceSCI  txRxDistanceTB  \n",
       "freq     2152319.0  11424.0               39              39  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
