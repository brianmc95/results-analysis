{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from itertools import repeat\n",
    "import datetime\n",
    "import time\n",
    "from natsort import natsorted\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tempfile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vectors(json_fields, single=False):\n",
    "    # Simple function to remove the vector from results in json file might be we remove that part from the json file\n",
    "    if single:\n",
    "        return json_fields.replace(\":vector\", \"\")\n",
    "    for i in range(len(json_fields)):\n",
    "        json_fields[i] = json_fields[i].replace(\":vector\", \"\")\n",
    "    return json_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(lower_bound, width, quantity):\n",
    "    \"\"\" create_bins returns an equal-width (distance) partitioning.\n",
    "        It returns an ascending list of tuples, representing the intervals.\n",
    "        A tuple bins[i], i.e. (bins[i][0], bins[i][1])  with i > 0\n",
    "        and i < quantity, satisfies the following conditions:\n",
    "            (1) bins[i][0] + width == bins[i][1]\n",
    "            (2) bins[i-1][0] + width == bins[i][0] and\n",
    "                bins[i-1][1] + width == bins[i][1]\n",
    "    \"\"\"\n",
    "    bins = []\n",
    "    for low in range(lower_bound, lower_bound + quantity * width + 1, width):\n",
    "        bins.append((low, low + width))\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_fields(df, fields, bin_width=10, bin_quantity=49):\n",
    "    \"\"\"\n",
    "    Bins multiple dfs into a single dictionary that can be used as an average for multiple fields across multiple\n",
    "    runs\n",
    "    :param df: dataframe to bin\n",
    "    :param fields: fields to be binned.\n",
    "    :param bin_width: width of each bin\n",
    "    :param bin_quantity: total number of bins\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bins = create_bins(lower_bound=0, width=bin_width, quantity=bin_quantity)\n",
    "    distances = []\n",
    "    overall_fields = {}\n",
    "    for interval in bins:\n",
    "        upper_b = interval[1]\n",
    "        distances.append(upper_b)\n",
    "\n",
    "    for field in fields:\n",
    "        print(\"{} being binned\".format(field))\n",
    "        overall_fields[field] = []\n",
    "\n",
    "    overall_fields[\"distance\"] = distances\n",
    "\n",
    "    distance_col = config[\"results\"][\"distance\"]\n",
    "\n",
    "    for i in range(len(bins)):\n",
    "        lower_b = bins[i][0]\n",
    "        upper_b = bins[i][1]\n",
    "        fields_temp = df[(df[distance_col] >= lower_b) & (df[distance_col] < upper_b)]\n",
    "        for field in fields:\n",
    "            if i < len(overall_fields[field]):\n",
    "                overall_fields[field][i] = (fields_temp[field].mean() + overall_fields[field][i]) / 2\n",
    "            else:\n",
    "                overall_fields[field].append(fields_temp[field].mean())\n",
    "\n",
    "    return overall_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_results(combined, results):\n",
    "    for result in results:\n",
    "        for field in result:\n",
    "            if field in combined:\n",
    "                for i in range(len(result[field])):\n",
    "                    combined[field][i] = (combined[field][i] + result[field][i]) / 2\n",
    "            else:\n",
    "                combined[field] = result[field]\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New parsing system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vector_desc_line(line):\n",
    "    try:\n",
    "        # Converts a vector description line to a dictionary for use in parsing later\n",
    "        node_id_pattern = re.compile(\"\\[\\d+\\]\")\n",
    "\n",
    "        vector_line_dict = {\"nodeID\": None, \"vectorName\": None, \"ETV\": True}\n",
    "        split_line = line.split(\" \")\n",
    "        vector_num = int(split_line[1])\n",
    "        match = node_id_pattern.search(split_line[2])\n",
    "        nodeID = int(match.group().strip(\"[]\"))\n",
    "        vector_name = split_line[3]\n",
    "        vector_name = vector_name.split(\":\")[0]\n",
    "        if \"ETV\" in split_line[4]:\n",
    "            ETV = True\n",
    "        else:\n",
    "            ETV = False\n",
    "\n",
    "        vector_line_dict[\"nodeID\"] = nodeID\n",
    "        vector_line_dict[\"vectorName\"] = vector_name\n",
    "        vector_line_dict[\"ETV\"] = ETV\n",
    "\n",
    "        return vector_num, vector_line_dict\n",
    "    except AttributeError as e:\n",
    "        print(\"Line: {} : Could not be parsed\".format(line))\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vector_line(line):\n",
    "    # Simple function to split a vector line and convert to floats.\n",
    "    try:\n",
    "        line = bytes(line, 'utf-8').decode('utf-8', 'ignore')\n",
    "        split_nums = line.split()\n",
    "        for i in range(len(split_nums)):\n",
    "            split_nums[i] = float(split_nums[i])\n",
    "        return split_nums\n",
    "    except ValueError as e:\n",
    "        print(\"Line: {} could not be converted due to bad encoding\".format(line))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_csv_line(vector_dict, vector_id, parsed_vec):\n",
    "    # Parses the vector line information to be written to the csv file.\n",
    "    node_id = vector_dict[vector_id][\"nodeID\"]\n",
    "    vector_name = vector_dict[vector_id][\"vectorName\"]\n",
    "    if vector_dict[vector_id][\"ETV\"]:\n",
    "        time = parsed_vec[2]\n",
    "        value = parsed_vec[3]\n",
    "    else:\n",
    "        time = parsed_vec[1]\n",
    "        value = parsed_vec[2]\n",
    "\n",
    "    csv_line = [node_id, time, vector_name, value]\n",
    "    return csv_line, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_chunk_writer(output_file, chunk_num, title_line):\n",
    "    # Setup our chunk writer\n",
    "\n",
    "    # First create a folder to hold chunks\n",
    "    chunk_folder = output_file.split(\".\")[0]\n",
    "    os.makedirs(chunk_folder, exist_ok=True)\n",
    "    chunk_name = \"{}/chunk-{}.csv\".format(chunk_folder, chunk_num)\n",
    "\n",
    "    # Create the chunk file and create a csv writer which uses it\n",
    "    print(\"Setting up new chunk: {}\".format(chunk_name))\n",
    "    temp_file_pt = open(chunk_name, \"w+\")\n",
    "    output_writer = csv.writer(temp_file_pt, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    output_writer.writerow(title_line)\n",
    "\n",
    "    return temp_file_pt, output_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vector_file(output_file, vector_path, stats, chunk_size=1e+8):\n",
    "    \"\"\"\n",
    "    chunk_size: the time between different files 1.5s as default\n",
    "    \"\"\"\n",
    "    # Reads the csv file, parses it and writes to a temp file for use later in generating a DF and CSV file.\n",
    "    vector_dict = {}\n",
    "    no_interest_vectors = {}  # Probably don't need to remember one's we don't care for.\n",
    "\n",
    "    chunk_times = []\n",
    "    chunk_info = {}\n",
    "\n",
    "    last_time = -1\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    # Patterns which identify vector declaration lines and result lines\n",
    "    vector_dec_line_pattern = re.compile(\"^vector\")\n",
    "    vector_res_line_pattern = re.compile(\"^\\d+\")\n",
    "\n",
    "    vector_file = open(vector_path, \"r\")\n",
    "\n",
    "    # Stores lines appearing before their declaration. Files are oddly formatted, this is purely safety ensuring we\n",
    "    # don't accidentally miss anything.\n",
    "    early_vectors = tempfile.NamedTemporaryFile(mode=\"r+\")\n",
    "\n",
    "    # Prepare and write out first line format NodeID, EventNumber, Time, Stat1, Stat2, Stat3, ...\n",
    "    title_line = [\"NodeID\", \"Time\", \"StatisticName\", \"Value\"]\n",
    "\n",
    "    temp_file_pt, writer = setup_chunk_writer(output_file, current_chunk_index, title_line)\n",
    "    chunk_info[\"CurrentChunk\"] = {\"file\": temp_file_pt, \"writer\": writer}\n",
    "\n",
    "    for line in vector_file:\n",
    "        if vector_dec_line_pattern.match(line):\n",
    "            # if line matches a vector declaration, parse the vector description\n",
    "            vector_num, vec_dict = parse_vector_desc_line(line)\n",
    "            if vector_num is None and vec_dict is None:\n",
    "                continue\n",
    "            if vec_dict[\"vectorName\"] in stats:\n",
    "                # Vector is of interest, add it to our overall dictionary and update it's index.\n",
    "                vector_dict[vector_num] = vec_dict\n",
    "            else:\n",
    "                # Mark this as a vector we don't care about.\n",
    "                no_interest_vectors[vector_num] = None\n",
    "\n",
    "        elif vector_res_line_pattern.match(line):\n",
    "            # {\"nodeID\": None, \"vectorName\": None, \"ETV\": True} This is what it looks like\n",
    "            parsed_vec = parse_vector_line(line)\n",
    "            # If the previous step fails then we can simply continue to the next line ignoring this line.\n",
    "            if parsed_vec is None:\n",
    "                continue\n",
    "            vector_id = parsed_vec[0]\n",
    "            if vector_id in vector_dict:\n",
    "                # Write out to a csv file correctly\n",
    "                csv_line, time = prepare_csv_line(vector_dict, vector_id, parsed_vec)\n",
    "\n",
    "                if time > last_time:\n",
    "                    chunk_info[\"CurrentChunk\"][\"writer\"].writerow(csv_line)\n",
    "                    if chunk_info[\"CurrentChunk\"][\"file\"].tell() >= chunk_size:\n",
    "                        print(\"Time ending this chunk:{}\".format(time))\n",
    "\n",
    "                        # This chunk is old and as such can be placed into the previous chunks\n",
    "                        chunk_info[time] = {\"file\": chunk_info[\"CurrentChunk\"][\"file\"],\n",
    "                                            \"writer\": chunk_info[\"CurrentChunk\"][\"writer\"]}\n",
    "                        chunk_times.append(time)\n",
    "                        last_time = time\n",
    "                        current_chunk_index += 1\n",
    "\n",
    "                        # This file is at max size, create a new writer\n",
    "                        temp_file_pt, writer = setup_chunk_writer(output_file, current_chunk_index, title_line)\n",
    "                        # Update current chunk writer to point at this new one.\n",
    "                        chunk_info[\"CurrentChunk\"] = {\"file\": temp_file_pt, \"writer\": writer}\n",
    "                if time <= last_time:\n",
    "                    for chunk_time in chunk_times:\n",
    "                        if time < chunk_time:\n",
    "                            chunk_info[chunk_time][\"writer\"].writerow(csv_line)\n",
    "            else:\n",
    "                if vector_id not in no_interest_vectors:\n",
    "                    # Write the line out in case we found it before declaration. Only if it is of possible interest.\n",
    "                    early_vectors.write(line)\n",
    "\n",
    "    # Rewind the early vectors file so we can search it for missed vectors\n",
    "    early_vectors.seek(0)\n",
    "\n",
    "    for line in early_vectors:\n",
    "        print(\"We have early vectors\")\n",
    "        # Parse the line again.\n",
    "        parsed_vec = parse_vector_line(line)\n",
    "        vector_id = parsed_vec[0]\n",
    "        # check for the vector\n",
    "        if vector_id in vector_dict:\n",
    "            # If we have it create the csv line and write it our\n",
    "            csv_line, time = prepare_csv_line(vector_dict, vector_id, parsed_vec)\n",
    "            for chunk_time in chunk_times:\n",
    "                if time < chunk_time:\n",
    "                    chunk_info[chunk_time][\"writer\"].writerow(csv_line)\n",
    "\n",
    "    # Close our vector file.\n",
    "    vector_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_pivot(directory, stats):\n",
    "    orig_loc = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "\n",
    "    csv_files = os.listdir(os.getcwd())\n",
    "    csv_files = natsorted(csv_files)\n",
    "    header = True\n",
    "    for csv_file in csv_files:\n",
    "        if \".csv\" in csv_file:\n",
    "            print(\"Dealing with chunk file: {}\".format(csv_file))\n",
    "            chunk_df = pd.read_csv(csv_file)\n",
    "\n",
    "            chunk_df = chunk_df.infer_objects()\n",
    "\n",
    "            chunk_df = chunk_df.sort_values(by=[\"NodeID\", \"Time\"])\n",
    "            # Parse the vector file to ensure it is formatted correclty.\n",
    "            chunk_df['seq'] = chunk_df.groupby([\"Time\", \"NodeID\", \"StatisticName\"]).cumcount()\n",
    "\n",
    "            chunk_df = chunk_df.pivot_table(\"Value\", [\"Time\", \"NodeID\", \"seq\"], \"StatisticName\")\n",
    "            chunk_df.reset_index(inplace=True)\n",
    "            chunk_df = chunk_df.drop([\"seq\"], axis=1)\n",
    "\n",
    "            # Ensure all fields correctly filled\n",
    "            for field in stats:\n",
    "                if field not in chunk_df.columns:\n",
    "                    chunk_df[field] = np.nan\n",
    "                    \n",
    "                    \n",
    "            chunk_df = chunk_df.reindex(sorted(chunk_df.columns), axis=1)\n",
    "\n",
    "            chunk_df.to_csv(csv_file, index=False, header=header)\n",
    "            header = False\n",
    "\n",
    "            del chunk_df\n",
    "\n",
    "    os.chdir(orig_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(csv_directory, outfile):\n",
    "    destination = open(outfile,'wb')\n",
    "    \n",
    "    orig_loc = os.getcwd()\n",
    "    os.chdir(csv_directory)\n",
    "    \n",
    "    csv_files = os.listdir()\n",
    "    csv_files = natsorted(csv_files)\n",
    "    header = True\n",
    "    for csv_file in csv_files:\n",
    "        if \".csv\" in csv_file and csv_file != outfile:\n",
    "            print(\"Dealing with chunk file: {}\".format(csv_file))\n",
    "            shutil.copyfileobj(open(csv_file,'rb'), destination)\n",
    "            os.remove(csv_file)\n",
    "    destination.close()\n",
    "    \n",
    "    os.chdir(orig_loc)\n",
    "    \n",
    "    os.rmdir(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dir = \"/Users/brianmccarthy/git_repos/results-analysis/data/omnet/cv2x/test\"\n",
    "\n",
    "vector_file_name_long = \"run-1.vec\"\n",
    "vector_file_name_short = \"short.vec\"\n",
    "\n",
    "vector_path_long = vector_dir + vector_file_name_long\n",
    "vector_path_short = vector_dir + vector_file_name_short\n",
    "\n",
    "config_name_long = \"long-test\"\n",
    "config_name_short = \"short-test\"\n",
    "\n",
    "experiment_type = \"cv2x\"\n",
    "\n",
    "json_path = \"/Users/brianmccarthy/git_repos/results-analysis/configs/cv2x.json\"\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "orig_loc = \"/Users/brianmccarthy/git_repos/results-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = \"long.csv\"\n",
    "real_vector_path = \"run-1.vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for dealing with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_path, \"r\") as json_file:\n",
    "    config = json.load(json_file)[\"cv2x\"]\n",
    "    json_fields = config[\"results\"]\n",
    "    \n",
    "real_vector_path = \"/Users/brianmccarthy/git_repos/results-analysis/data/omnet/cv2x/test/run-1.vec\"\n",
    "output_csv = \"/Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_data(real_vector_path, json_fields, output_csv):\n",
    "    # Simply remove the :vector part of vector names from both sets of vectors.\n",
    "    found_vector = False\n",
    "    for field in json_fields:\n",
    "        if \":vector\" in field:\n",
    "            found_vector = True\n",
    "            break\n",
    "\n",
    "    if found_vector:\n",
    "        json_fields = remove_vectors(json_fields)\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "    start_time = overall_start_time\n",
    "    print(\"Beginning reading of vector file: {}\".format(real_vector_path))\n",
    "\n",
    "    # Read the vector file into a csv file\n",
    "    chunk_folder = output_csv.split(\".\")[0]\n",
    "    read_vector_file(output_csv, real_vector_path, json_fields)\n",
    "    \n",
    "    print(\"Finished reading of vector file: {} in {}s\".format(real_vector_path, time.time() - start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"File read, begin pivoting csv file: {}\".format(real_vector_path))\n",
    "    \n",
    "    \n",
    "    print(\"Finished pivoting csv file: {} in {}s\".format(real_vector_path, time.time() - start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Pivot complete, consolidate chunk files for {}\".format(output_csv))\n",
    "    combine_files(chunk_folder, output_csv)\n",
    "\n",
    "    print(\"Finished parsing of vector file: {} in {}s\".format(real_vector_path, time.time() - overall_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_data(real_vector_path, json_fields[\"filtered_vectors\"], output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-0.csv\n",
      "Time ending this chunk:500.806\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-1.csv\n",
      "Time ending this chunk:501.134\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-2.csv\n",
      "Time ending this chunk:501.419\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-3.csv\n",
      "Time ending this chunk:501.668\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-4.csv\n",
      "Time ending this chunk:501.947\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-5.csv\n",
      "Time ending this chunk:502.225\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-6.csv\n",
      "Time ending this chunk:502.472\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-7.csv\n",
      "Time ending this chunk:502.767\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-8.csv\n",
      "Time ending this chunk:503.01\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-9.csv\n",
      "Time ending this chunk:503.311\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-10.csv\n",
      "Time ending this chunk:503.554\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-11.csv\n",
      "Time ending this chunk:503.84\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-12.csv\n",
      "Time ending this chunk:504.085\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-13.csv\n",
      "Time ending this chunk:504.372\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-14.csv\n",
      "Time ending this chunk:504.639\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-15.csv\n",
      "Time ending this chunk:504.885\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-16.csv\n",
      "Time ending this chunk:505.147\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-17.csv\n",
      "Time ending this chunk:505.442\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-18.csv\n",
      "Time ending this chunk:505.693\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-19.csv\n",
      "Time ending this chunk:505.956\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-20.csv\n",
      "Time ending this chunk:506.251\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-21.csv\n",
      "Time ending this chunk:506.501\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-22.csv\n",
      "Time ending this chunk:506.804\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-23.csv\n",
      "Time ending this chunk:507.065\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-24.csv\n",
      "Time ending this chunk:507.324\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-25.csv\n",
      "Time ending this chunk:507.576\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-26.csv\n",
      "Time ending this chunk:507.848\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-27.csv\n",
      "Time ending this chunk:508.131\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-28.csv\n",
      "Time ending this chunk:508.38\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-29.csv\n",
      "Time ending this chunk:508.692\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-30.csv\n",
      "Time ending this chunk:508.968\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-31.csv\n",
      "Time ending this chunk:509.218\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-32.csv\n",
      "Time ending this chunk:509.505\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-33.csv\n",
      "Time ending this chunk:509.755\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-34.csv\n",
      "Time ending this chunk:510.013\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-35.csv\n",
      "Time ending this chunk:510.316\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-36.csv\n",
      "Time ending this chunk:510.586\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-37.csv\n",
      "Time ending this chunk:510.854\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-38.csv\n",
      "Time ending this chunk:511.133\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-39.csv\n",
      "Time ending this chunk:511.403\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-40.csv\n",
      "Time ending this chunk:511.674\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-41.csv\n",
      "Time ending this chunk:511.922\n",
      "Setting up new chunk: /Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test/chunk-42.csv\n"
     ]
    }
   ],
   "source": [
    "chunk_folder = output_csv.split(\".\")[0]\n",
    "read_vector_file(output_csv, real_vector_path, json_fields[\"filtered_vectors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with chunk file: chunk-0.csv\n",
      "Dealing with chunk file: chunk-1.csv\n",
      "Dealing with chunk file: chunk-2.csv\n",
      "Dealing with chunk file: chunk-3.csv\n",
      "Dealing with chunk file: chunk-4.csv\n",
      "Dealing with chunk file: chunk-5.csv\n",
      "Dealing with chunk file: chunk-6.csv\n",
      "Dealing with chunk file: chunk-7.csv\n",
      "Dealing with chunk file: chunk-8.csv\n",
      "Dealing with chunk file: chunk-9.csv\n",
      "Dealing with chunk file: chunk-10.csv\n",
      "Dealing with chunk file: chunk-11.csv\n",
      "Dealing with chunk file: chunk-12.csv\n",
      "Dealing with chunk file: chunk-13.csv\n",
      "Dealing with chunk file: chunk-14.csv\n",
      "Dealing with chunk file: chunk-15.csv\n",
      "Dealing with chunk file: chunk-16.csv\n",
      "Dealing with chunk file: chunk-17.csv\n",
      "Dealing with chunk file: chunk-18.csv\n",
      "Dealing with chunk file: chunk-19.csv\n",
      "Dealing with chunk file: chunk-20.csv\n",
      "Dealing with chunk file: chunk-21.csv\n",
      "Dealing with chunk file: chunk-22.csv\n",
      "Dealing with chunk file: chunk-23.csv\n",
      "Dealing with chunk file: chunk-24.csv\n",
      "Dealing with chunk file: chunk-25.csv\n",
      "Dealing with chunk file: chunk-26.csv\n",
      "Dealing with chunk file: chunk-27.csv\n",
      "Dealing with chunk file: chunk-28.csv\n",
      "Dealing with chunk file: chunk-29.csv\n",
      "Dealing with chunk file: chunk-30.csv\n",
      "Dealing with chunk file: chunk-31.csv\n",
      "Dealing with chunk file: chunk-32.csv\n",
      "Dealing with chunk file: chunk-33.csv\n",
      "Dealing with chunk file: chunk-34.csv\n",
      "Dealing with chunk file: chunk-35.csv\n",
      "Dealing with chunk file: chunk-36.csv\n",
      "Dealing with chunk file: chunk-37.csv\n",
      "Dealing with chunk file: chunk-38.csv\n",
      "Dealing with chunk file: chunk-39.csv\n",
      "Dealing with chunk file: chunk-40.csv\n",
      "Dealing with chunk file: chunk-41.csv\n",
      "Dealing with chunk file: chunk-42.csv\n"
     ]
    }
   ],
   "source": [
    "csv_pivot(chunk_folder, json_fields[\"filtered_vectors\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with chunk file: chunk-0.csv\n",
      "Dealing with chunk file: chunk-1.csv\n",
      "Dealing with chunk file: chunk-2.csv\n",
      "Dealing with chunk file: chunk-3.csv\n",
      "Dealing with chunk file: chunk-4.csv\n",
      "Dealing with chunk file: chunk-5.csv\n",
      "Dealing with chunk file: chunk-6.csv\n",
      "Dealing with chunk file: chunk-7.csv\n",
      "Dealing with chunk file: chunk-8.csv\n",
      "Dealing with chunk file: chunk-9.csv\n",
      "Dealing with chunk file: chunk-10.csv\n",
      "Dealing with chunk file: chunk-11.csv\n",
      "Dealing with chunk file: chunk-12.csv\n",
      "Dealing with chunk file: chunk-13.csv\n",
      "Dealing with chunk file: chunk-14.csv\n",
      "Dealing with chunk file: chunk-15.csv\n",
      "Dealing with chunk file: chunk-16.csv\n",
      "Dealing with chunk file: chunk-17.csv\n",
      "Dealing with chunk file: chunk-18.csv\n",
      "Dealing with chunk file: chunk-19.csv\n",
      "Dealing with chunk file: chunk-20.csv\n",
      "Dealing with chunk file: chunk-21.csv\n",
      "Dealing with chunk file: chunk-22.csv\n",
      "Dealing with chunk file: chunk-23.csv\n",
      "Dealing with chunk file: chunk-24.csv\n",
      "Dealing with chunk file: chunk-25.csv\n",
      "Dealing with chunk file: chunk-26.csv\n",
      "Dealing with chunk file: chunk-27.csv\n",
      "Dealing with chunk file: chunk-28.csv\n",
      "Dealing with chunk file: chunk-29.csv\n",
      "Dealing with chunk file: chunk-30.csv\n",
      "Dealing with chunk file: chunk-31.csv\n",
      "Dealing with chunk file: chunk-32.csv\n",
      "Dealing with chunk file: chunk-33.csv\n",
      "Dealing with chunk file: chunk-34.csv\n",
      "Dealing with chunk file: chunk-35.csv\n",
      "Dealing with chunk file: chunk-36.csv\n",
      "Dealing with chunk file: chunk-37.csv\n",
      "Dealing with chunk file: chunk-38.csv\n",
      "Dealing with chunk file: chunk-39.csv\n",
      "Dealing with chunk file: chunk-40.csv\n",
      "Dealing with chunk file: chunk-41.csv\n",
      "Dealing with chunk file: chunk-42.csv\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 66] Directory not empty: '/Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-361f984be8a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombine_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-eb4e1c496f4e>\u001b[0m in \u001b[0;36mcombine_files\u001b[0;34m(csv_directory, outfile)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 66] Directory not empty: '/Users/brianmccarthy/git_repos/results-analysis/data/raw_data/cv2x/test/test'"
     ]
    }
   ],
   "source": [
    "combine_files(chunk_folder, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
