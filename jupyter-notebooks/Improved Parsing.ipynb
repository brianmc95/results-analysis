{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Parsing\n",
    "\n",
    "This notebook is designed to look at the mechanisms used in parsing the results of simulation runs and ensuring they are done in a memory efficent way.\n",
    "\n",
    "- Need to read the files in chunks\n",
    "- Combine the result of each chunk into a single result\n",
    "- Condense the results across files into a single result (this will be done in the actual script not jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markers to use for this run\n",
    "markers = [\".\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\", \"p\", \"P\", \"*\", \"h\",\n",
    "           \"H\", \"+\", \"x\", \"X\", \"D\", \"d\", \"|\", \"_\", 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON file containing the results for this simulation run\n",
    "configuration_file = \"/Users/brianmccarthy/git_repos/results-analysis/configs/cv2x.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(configuration_file) as config_json:\n",
    "    config = json.load(config_json)[\"cv2x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(lower_bound, width, quantity):\n",
    "    \"\"\" create_bins returns an equal-width (distance) partitioning.\n",
    "        It returns an ascending list of tuples, representing the intervals.\n",
    "        A tuple bins[i], i.e. (bins[i][0], bins[i][1])  with i > 0\n",
    "        and i < quantity, satisfies the following conditions:\n",
    "            (1) bins[i][0] + width == bins[i][1]\n",
    "            (2) bins[i-1][0] + width == bins[i][0] and\n",
    "                bins[i-1][1] + width == bins[i][1]\n",
    "    \"\"\"\n",
    "    bins = []\n",
    "    for low in range(lower_bound, lower_bound + quantity * width + 1, width):\n",
    "        bins.append((low, low + width))\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_fields(df, fields, bin_width=10, bin_quantity=49):\n",
    "    \"\"\"\n",
    "    Bins multiple dfs into a single dictionary that can be used as an average for multiple fields across multiple\n",
    "    runs\n",
    "    :param df: dataframe to bin\n",
    "    :param fields: fields to be binned.\n",
    "    :param bin_width: width of each bin\n",
    "    :param bin_quantity: total number of bins\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    bins = create_bins(lower_bound=0, width=bin_width, quantity=bin_quantity)\n",
    "    distances = []\n",
    "    overall_fields = {}\n",
    "    for interval in bins:\n",
    "        upper_b = interval[1]\n",
    "        distances.append(upper_b)\n",
    "\n",
    "    for field in fields:\n",
    "        overall_fields[field] = []\n",
    "\n",
    "    overall_fields[\"distance\"] = distances\n",
    "\n",
    "    distance_col = config[\"results\"][\"distance\"]\n",
    "\n",
    "    for i in range(len(bins)):\n",
    "        lower_b = bins[i][0]\n",
    "        upper_b = bins[i][1]\n",
    "        fields_temp = df[(df[distance_col] >= lower_b) & (df[distance_col] < upper_b)]\n",
    "        for field in fields:\n",
    "            overall_fields[field].append(fields_temp[field].mean())\n",
    "\n",
    "    return overall_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdr_dist(pdrs, distances, labels, plot_name, show=True, store=False):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(len(pdrs)):\n",
    "        ax.plot(distances, pdrs[i], label=labels[i])\n",
    "\n",
    "    ax.set(xlabel='Distance (m)', ylabel='Packet Delivery Rate (PDR) %')\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.tick_params(direction='in')\n",
    "    \n",
    "    ax.set_ylim([0, 100])\n",
    "    plt.yticks(np.arange(0, 101, step=10))\n",
    "\n",
    "    ax.set_xlim([0, (max(distances) + 1)])\n",
    "    plt.xticks(np.arange(0, (max(distances) + 1), step=50))\n",
    "\n",
    "    fig.suptitle(plot_name, fontsize=12)\n",
    "    \n",
    "    if show:\n",
    "        fig.show()\n",
    "        \n",
    "    if store:\n",
    "        fig.savefig(\"{}.png\".format(plot_name), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do:\n",
    "1. Figure out where the files are.\n",
    "2. Read each file in chunks.\n",
    "3. For each chunk do your calculation on the statistic (e.g. calc pdr @ distances/average messageLatency)\n",
    "4. Combine the results from each chunk into a single average across the file.\n",
    "5. Combine the results across the files into a single average across the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(folder):\n",
    "    # 1) Figure out where the files are.\n",
    "    overall_results = pd.DataFrame()\n",
    "\n",
    "    bins = create_bins(lower_bound=0, width=10, quantity=49)\n",
    "    distances = []\n",
    "    overall_fields = {}\n",
    "    for interval in bins:\n",
    "        upper_b = interval[1]\n",
    "        distances.append(upper_b)\n",
    "\n",
    "    recorded_results = pd.DataFrame()\n",
    "    for file in natsorted(os.listdir(folder)):\n",
    "        if \".csv\" in file:\n",
    "            print(\"Dealing with file:{}\".format(file))\n",
    "\n",
    "            file_path = os.path.join(folder, file)\n",
    "            # 2) Read each file in chunks.\n",
    "            # Tell pandas to read the data in chunks\n",
    "            chunks = pd.read_csv(file_path, chunksize=1e6)\n",
    "\n",
    "            chunk_count = 0\n",
    "            for chunk in chunks:\n",
    "                # 3) For each chunk do your calculation on the statistic \n",
    "                #    (e.g. calc pdr @ distances/average messageLatency)\n",
    "                # Filter the times down\n",
    "                if chunk[\"Time\"].max() < 502:\n",
    "                    # Skip until 502\n",
    "#                     print(\"Chunk not far enough into file to use results\")\n",
    "                    continue\n",
    "\n",
    "                chunk = chunk[chunk[\"Time\"] > 502]\n",
    "\n",
    "                # Calculate pdr\n",
    "                binned_fields = bin_fields(chunk, [\"tbDecoded\"])\n",
    "\n",
    "                # 4) Combine the results from each chunk into a single average across the file.\n",
    "                chunk_res = pd.DataFrame([binned_fields[\"tbDecoded\"]],columns=distances)\n",
    "                if recorded_results.empty:\n",
    "                    recorded_results = chunk_res\n",
    "                else:\n",
    "                    recorded_results.append(chunk_res)\n",
    "                    \n",
    "                    \n",
    "    # 5) Combine the results across the files into a single average across the folder.\n",
    "    file_mean = pd.DataFrame(recorded_results.mean().to_dict(),index=[recorded_results.index.values[-1]])\n",
    "    if overall_results.empty:\n",
    "        overall_results = file_mean\n",
    "    else:\n",
    "        overall_results = overall_results.append(file_mean)\n",
    "    print(\"Combine all the files in this folder\")\n",
    "    return overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for folder in config[\"raw-results\"]:\n",
    "    if \"Analytical-Half\" in folder:\n",
    "        print(\"Dealing with folder: {}\".format(folder))\n",
    "        results[os.path.basename(folder)] = parse_results(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdrs = []\n",
    "for key in results:\n",
    "    if key == \"Analytical-Half\":\n",
    "#         continue\n",
    "        df = results[key]\n",
    "        print(key)\n",
    "        pdrs.append(df.values.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = create_bins(lower_bound=0, width=10, quantity=49)\n",
    "distances = []\n",
    "overall_fields = {}\n",
    "for interval in bins:\n",
    "    upper_b = interval[1]\n",
    "    distances.append(upper_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdrs)):\n",
    "    for j in range(len(pdrs[i])):\n",
    "        pdrs[i][j] = pdrs[i][j] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdr_dist(pdrs, distances, [\"Analytical\"], \"Analytical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
