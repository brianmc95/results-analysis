{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import multiprocessing\n",
    "import json\n",
    "import re\n",
    "\n",
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c8c8f57c82ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m            \"x\", \"X\", \"D\", \"d\", \"|\", \"_\", 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"confidence-interval\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moverall_now\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"12:00:00\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "use_markers = False\n",
    "experiment_type = \"cv2x\"\n",
    "\n",
    "use_line_types = False\n",
    "image_format = \"png\"\n",
    "figure_store = \"../data/figures/\"\n",
    "\n",
    "markers = [\".\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\", \"p\", \"P\", \"*\", \"h\", \"H\", \"+\",\n",
    "           \"x\", \"X\", \"D\", \"d\", \"|\", \"_\", 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "p = results[\"confidence-interval\"]\n",
    "\n",
    "overall_now=\"12:00:00\"\n",
    "confidence_intervals = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner for overall job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graphs(result_folders, now):\n",
    "\n",
    "    print(\"Beginning graphing of result file: {}\".format(result_folders))\n",
    "\n",
    "    if not config[\"processed-result-dir\"]:\n",
    "        config[\"processed-result-dir\"] = prepare_results(result_folders, now)\n",
    "\n",
    "    for graph_title in results[\"graph-configurations\"]:\n",
    "        print(\"Graphing configuration: {}\".format(graph_title))\n",
    "        folders_for_comparison = []\n",
    "        configurations = []\n",
    "        for configuration in results[\"graph-configurations\"][graph_title]:\n",
    "            for folder in config[\"processed-result-dir\"]:\n",
    "                config_name = folder.split(\"/\")[-1][:-9]\n",
    "                if configuration == config_name:\n",
    "                    folders_for_comparison.append(folder)\n",
    "                    configurations.append(configuration)\n",
    "\n",
    "        for graph in results[\"graphs\"]:\n",
    "            if graph in [\"PDR-SCI\", \"PDR-TB\", \"IPG\"]:\n",
    "                distance_graph(folders_for_comparison, graph, graph_title, configurations, now)\n",
    "            elif graph == \"CBR\":\n",
    "                cbr_graph(folders_for_comparison, graph, graph_title, configurations, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Preparation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(result_folders, now):\n",
    "\n",
    "    num_processes = config[\"parallel_processes\"]\n",
    "    if num_processes > multiprocessing.cpu_count():\n",
    "        print(\"Too many processes, going to revert to total - 1\")\n",
    "        num_processes = multiprocessing.cpu_count() - 1\n",
    "\n",
    "    processed_results = []\n",
    "    for folder in result_folders:\n",
    "        config_name = folder.split(\"/\")[-1]\n",
    "        print(\"Results for config: {}\".format(config_name))\n",
    "        folder_results = []\n",
    "        files = natsort.natsorted(os.listdir(folder))\n",
    "        \n",
    "        filtered_files = []\n",
    "        for i in range(len(files)):\n",
    "            if \".csv\" in files[i]:\n",
    "                filtered_files.append(\"{}/{}\".format(folder, files[i]))\n",
    "\n",
    "        i = 0\n",
    "        while i < len(filtered_files):\n",
    "            if len(filtered_files) < num_processes:\n",
    "                num_processes = len(filtered_files)\n",
    "            pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "            folder_results.append(pool.starmap(generate_results, zip(filtered_files[i: i + num_processes])))\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            i += num_processes\n",
    "\n",
    "        folder_results = [y for x in folder_results for y in x]\n",
    "        # Go through each of the available stats and write them out to a csv file.\n",
    "        output_csv_dir = \"../data/processed_data/{}/{}-{}\".format(experiment_type,\n",
    "                                                                  config_name, now)\n",
    "\n",
    "        os.makedirs(output_csv_dir, exist_ok=True)\n",
    "\n",
    "        # Shortcut ensures we get the stats from the parsed results\n",
    "        for stat in folder_results[0]:\n",
    "            if \"SCI\" in stat:\n",
    "                across_run_results(folder_results, stat, output_csv_dir, \"txRxDistanceSCI\")\n",
    "            elif stat == \"CBR\":\n",
    "                across_run_results(folder_results, stat, output_csv_dir, \"Time\")\n",
    "            else:\n",
    "                across_run_results(folder_results, stat, output_csv_dir, \"txRxDistanceTB\")\n",
    "\n",
    "        processed_results.append(output_csv_dir)\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(output_csv):\n",
    "\n",
    "    print(\"Generating results for file: {}\".format(output_csv))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    pdr_sci_agg = pd.DataFrame()\n",
    "    pdr_tb_agg = pd.DataFrame()\n",
    "    ipg_agg = pd.DataFrame()\n",
    "    cbr_agg = pd.DataFrame()\n",
    "\n",
    "    for chunk in pd.read_csv(output_csv, chunksize=10 ** 6):\n",
    "\n",
    "        # SCI PDR calculation\n",
    "        pdr_sci_agg = stat_distance(pdr_sci_agg, chunk, \"sciDecoded\", \"txRxDistanceSCI\", True)\n",
    "\n",
    "        # TB PDR calculation\n",
    "        pdr_tb_agg = stat_distance(pdr_tb_agg, chunk, \"tbDecoded\", \"txRxDistanceTB\", True)\n",
    "\n",
    "        # IPG calculation\n",
    "        ipg_agg = stat_distance(ipg_agg, chunk, \"interPacketDelay\", \"txRxDistanceTB\", False)\n",
    "\n",
    "        # CBR calculation doesn't aggregate the same way as the above so dealt with separately\n",
    "        cbr_df = chunk[chunk[\"cbr\"].notnull()]\n",
    "        cbr_df = cbr_df[[\"Time\", \"cbr\"]]\n",
    "        cbr_df = cbr_df.groupby(\"Time\").agg({\"cbr\": [np.mean, np.std, \"count\"]})\n",
    "        cbr_df.columns = cbr_df.columns.droplevel()\n",
    "        cbr_df = cbr_df.apply(lambda x: x * 100, axis=1)\n",
    "\n",
    "        if cbr_agg.empty:\n",
    "            cbr_agg = cbr_df\n",
    "        else:\n",
    "            # combine_chunks\n",
    "            cbr_agg = cbr_agg.append(cbr_df)\n",
    "\n",
    "    results[\"PDR-SCI\"] = pdr_sci_agg\n",
    "    results[\"PDR-TB\"] = pdr_tb_agg\n",
    "    results[\"IPG\"] = ipg_agg\n",
    "    results[\"CBR\"] = cbr_agg\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_distance(agg_df, df, stat, distance, percentage):\n",
    "\n",
    "    # Reduce the size of the DF to what we're interested in.\n",
    "    distance_df = df[df[stat].notnull()]\n",
    "    distance_df = distance_df[[\"Time\", \"NodeID\", stat, distance]]\n",
    "    distance_df = distance_df[distance_df[stat] > -1]\n",
    "\n",
    "    max_distance = min(510, distance_df[distance].max())\n",
    "\n",
    "    # Get the mean, std, count for each distance\n",
    "    distance_df = distance_df.groupby(\n",
    "        pd.cut(distance_df[distance], np.arange(0, max_distance, 10))).agg(\n",
    "        {stat: [np.mean, \"count\"]})\n",
    "\n",
    "    # Remove over head column\n",
    "    distance_df.columns = distance_df.columns.droplevel()\n",
    "\n",
    "    if percentage:\n",
    "        distance_df = distance_df.apply(lambda x: x * 100, axis=1)\n",
    "\n",
    "    if agg_df.empty:\n",
    "        agg_df = distance_df\n",
    "    else:\n",
    "        # combine_chunks\n",
    "        agg_df = pd.merge(agg_df, distance_df, on=distance, how='outer')\n",
    "        agg_df = agg_df.apply(combine_line, axis=1, result_type='expand')\n",
    "        agg_df = agg_df.rename({0: \"mean\", 1: \"count\"}, axis='columns')\n",
    "\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_line(line):\n",
    "    mean_a = line[\"mean_x\"]\n",
    "    count_a = line[\"count_x\"]\n",
    "\n",
    "    mean_b = line[\"mean_y\"]\n",
    "    count_b = line[\"count_y\"]\n",
    "\n",
    "    if np.isnan(mean_a) and np.isnan(mean_b):\n",
    "        return [mean_a, count_a]\n",
    "    elif np.isnan(mean_a) and not np.isnan(mean_b):\n",
    "        return [mean_b, count_b]\n",
    "    elif np.isnan(mean_b) and not np.isnan(mean_a):\n",
    "        return [mean_a, count_a]\n",
    "    else:\n",
    "        ex_a = mean_a * count_a\n",
    "        ex_b = mean_b * count_b\n",
    "\n",
    "        tx = ex_a + ex_b\n",
    "        tn = count_a + count_b\n",
    "\n",
    "        overall_mean = tx / tn\n",
    "        overall_count = tn\n",
    "\n",
    "        return [overall_mean, overall_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def across_run_results(results, stat, output_csv_dir, merge_col):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    print(\"Statistic of interest: {}\".format(stat))\n",
    "    for i in range(len(results)):\n",
    "        if df.empty:\n",
    "            df = results[i][stat]\n",
    "        else:\n",
    "            df = pd.merge(df, results[i][stat], how='outer', on=merge_col,\n",
    "                          suffixes=(i, i + 1),\n",
    "                          copy=True, indicator=False)\n",
    "\n",
    "    mean_cols = df.filter(regex='mean').columns\n",
    "\n",
    "    n = len(mean_cols) - 1\n",
    "    t_value = t.ppf(p, n)\n",
    "\n",
    "    df = df.apply(combine_runs, axis=1, result_type='expand', args=(mean_cols, t_value,))\n",
    "    df = df.rename({0: \"Mean\", 1: \"Confidence-Interval\"}, axis='columns')\n",
    "    df.to_csv(\"{}/{}.csv\".format(output_csv_dir, stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_runs(line, mean_cols, t_value):\n",
    "    means = []\n",
    "    for mean in mean_cols:\n",
    "        means.append(line[mean])\n",
    "\n",
    "    n = len(means)\n",
    "\n",
    "    # Average Across runs\n",
    "    xBar = sum(means) / n\n",
    "\n",
    "    # Deviation between runs and average\n",
    "    deviation = []\n",
    "    for mean in means:\n",
    "        deviation.append((mean - xBar) ** 2)\n",
    "    s2 = sum(deviation) / (n - 1)\n",
    "\n",
    "    # Confidence interval\n",
    "    ci = t_value * math.sqrt(s2 / n)\n",
    "\n",
    "    return [xBar, ci]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_graph(folders, graph, comparison, configurations, now):\n",
    "    means = []\n",
    "    cis = []\n",
    "    distances = []\n",
    "    for folder, config in zip(folders, configurations):\n",
    "        df = pd.read_csv(\"{}/{}.csv\".format(folder, graph))\n",
    "        means.append(list(df[\"Mean\"]))\n",
    "        if confidence_intervals:\n",
    "            cis.append(list(df[\"Confidence-Interval\"]))\n",
    "        distances = (list(range(0, df.shape[0] * 10, 10)))\n",
    "\n",
    "    if graph in [\"PDR-SCI\", \"PDR-TB\"]:\n",
    "        dist_graph(means, distances, configurations,\n",
    "                   \"{}-{}\".format(comparison, graph), ylabel=\"Packet Delivery Rate %\", now=now,\n",
    "                    confidence_intervals=cis, show=False, store=True)\n",
    "    elif graph == \"IPG\":\n",
    "        dist_graph(means, distances, configurations,\n",
    "                   \"{}-{}\".format(comparison, graph), ylabel=\"Inter-Packet Gap (ms)\", now=now,\n",
    "                   legend_pos=\"upper left\", confidence_intervals=cis, show=False, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbr_graph(folders, graph, comparison, configurations, now):\n",
    "    # Might change this to time based graph but CBR is fine for now\n",
    "    times = []\n",
    "    cbr = []\n",
    "    cis = []\n",
    "    for folder, config in zip(folders, configurations):\n",
    "        df = pd.read_csv(\"{}/CBR.csv\".format(folder))\n",
    "        times.append(list(df[\"Time\"]))\n",
    "        cbr.append(list(df[\"Mean\"]))\n",
    "        if confidence_intervals:\n",
    "            cis.append(list(df[\"Confidence-Interval\"]))\n",
    "\n",
    "    cbr_plot(cbr, times, \"{}-{}\".format(comparison, graph), configurations, now=now,\n",
    "             confidence_intervals=cis, show=False, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_graph(means, distances, labels, plot_name, ylabel, now, legend_pos=\"lower left\",\n",
    "               confidence_intervals=None, show=True, store=False):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(len(means)):\n",
    "        if confidence_intervals:\n",
    "            ax.errorbar(distances, means[i], yerr=confidence_intervals[i], label=labels[i])\n",
    "        else:\n",
    "            ax.plot(distances, means[i], label=labels[i])\n",
    "\n",
    "    ax.set(xlabel='Distance (m)', ylabel=ylabel)\n",
    "    ax.legend(loc=legend_pos)\n",
    "    ax.tick_params(direction='in')\n",
    "\n",
    "    ax.set_xlim([0, (max(distances) + 1)])\n",
    "    plt.xticks(np.arange(0, (max(distances) + 1), step=50))\n",
    "\n",
    "#     fig.suptitle(plot_name, fontsize=12)\n",
    "\n",
    "    if show:\n",
    "        fig.show()\n",
    "\n",
    "    if store:\n",
    "        fig.savefig(\"{}/{}-{}.png\".format(figure_store, plot_name, now), dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbr_plot(cbr, times, plot_name, labels, now, confidence_intervals=None, show=True, store=False):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(len(cbr)):\n",
    "        if confidence_intervals:\n",
    "            ax.errorbar(times[i], cbr[i], yerr=confidence_intervals[i], label=labels[i])\n",
    "        else:\n",
    "            ax.plot(times[i], cbr[i], label=labels[i])\n",
    "\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set(xlabel='Time (s)', ylabel='Channel Busy Ratio %')\n",
    "    ax.tick_params(direction='in')\n",
    "\n",
    "    ax.set_ylim([0, 100])\n",
    "    plt.yticks(np.arange(0, 101, step=10))\n",
    "\n",
    "#     fig.suptitle(plot_name, fontsize=12)\n",
    "\n",
    "    if show:\n",
    "        fig.show()\n",
    "\n",
    "    if store:\n",
    "        fig.savefig(\"{}/{}-{}.png\".format(figure_store, plot_name, now), dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_dist(distances, decoded, decoded_labels, errors, error_labels, plot_name):\n",
    "    # TODO: Update to allow such graphing to be automatically configured.\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    if use_markers:\n",
    "        for i in range(len(decoded)):\n",
    "            ax.plot(distances, decoded[i], label=decoded_labels[i], marker=markers[i], markevery=3)\n",
    "\n",
    "            for j in range(len(errors[i])):\n",
    "                ax.plot(distances, errors[i][j], label=error_labels[i][j], marker=markers[i + j])\n",
    "\n",
    "    elif use_line_types:\n",
    "        for i in range(len(decoded)):\n",
    "            ax.plot(distances, decoded[i], label=decoded_labels[i])\n",
    "\n",
    "            for j in range(len(errors[i])):\n",
    "                ax.plot(distances, errors[i][j], label=error_labels[i][j])\n",
    "\n",
    "    else:\n",
    "        for i in range(len(decoded)):\n",
    "            ax.plot(distances, decoded[i], label=decoded_labels[i])\n",
    "\n",
    "            for j in range(len(errors[i])):\n",
    "                ax.plot(distances, errors[i][j], label=error_labels[i][j])\n",
    "\n",
    "    ax.legend(loc='center left')\n",
    "\n",
    "    ax.set(xlabel='Distance (m)', ylabel='Packet Delivery Rate (PDR) %')\n",
    "    ax.grid()\n",
    "\n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.yticks(np.arange(0, 1.1, step=.1))\n",
    "\n",
    "    ax.set_xlim([0, (max(distances) + 1)])\n",
    "    plt.xticks(np.arange(0, (max(distances) + 1), step=50))\n",
    "\n",
    "    fig.savefig(\"{}/{}-{}.png\".format(figure_store, plot_name, now), dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/Users/brianmccarthy/git_repos/results-analysis/configs/cv2x.json\"\n",
    "with open(config_file) as config_json:\n",
    "    config = json.load(config_json)[experiment_type]\n",
    "results = config[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning graphing of result file: []\n",
      "Graphing configuration: Motivation\n",
      "Graphing configuration: PacketDrop-MD\n",
      "Graphing configuration: PacketDrop-HD\n",
      "Graphing configuration: DCC\n",
      "Graphing configuration: RandomAccess\n",
      "Graphing configuration: RRI-Adaptation\n"
     ]
    }
   ],
   "source": [
    "generate_graphs(config[\"parsed-result-dir\"], overall_now)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
