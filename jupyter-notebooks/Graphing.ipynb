{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import logging\n",
    "import os\n",
    "import math\n",
    "import multiprocessing\n",
    "import json\n",
    "import re\n",
    "\n",
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_markers = False\n",
    "experiment_type = \"cv2x\"\n",
    "\n",
    "use_line_types = False\n",
    "image_format = \"png\"\n",
    "figure_store = \"../data/figures/\"\n",
    "\n",
    "overall_now=\"12:00:00\"\n",
    "confidence_intervals = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner for overall job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graphs(result_folders, now):\n",
    "\n",
    "    print(\"Beginning graphing of result file: {}\".format(result_folders))\n",
    "\n",
    "    if not config[\"processed-result-dir\"]:\n",
    "        config[\"processed-result-dir\"] = prepare_results(result_folders, now)\n",
    "\n",
    "    for graph_title in results_[\"graph-configurations\"]:\n",
    "        print(\"Graphing configuration: {}\".format(graph_title))\n",
    "        folders_for_comparison = []\n",
    "        configurations = []\n",
    "        for configuration in results_[\"graph-configurations\"][graph_title]:\n",
    "            for folder in config[\"processed-result-dir\"]:\n",
    "                config_name = folder.split(\"/\")[-1][:-20]\n",
    "                if configuration == config_name:\n",
    "                    folders_for_comparison.append(folder)\n",
    "                    configurations.append(configuration)\n",
    "\n",
    "        for graph in results_[\"graphs\"]:\n",
    "            if graph in [\"PDR-SCI\", \"PDR-TB\", \"IPG\"]:\n",
    "                distance_graph(folders_for_comparison, graph, graph_title, configurations, now)\n",
    "            elif graph == \"CBR\":\n",
    "                cbr_graph(folders_for_comparison, graph, graph_title, configurations, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Preparation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(result_folders, now):\n",
    "\n",
    "    num_processes = config[\"parallel_processes\"]\n",
    "    if num_processes > multiprocessing.cpu_count():\n",
    "        print(\"Too many processes, going to revert to total - 1\")\n",
    "        num_processes = multiprocessing.cpu_count() - 1\n",
    "\n",
    "    processed_results = []\n",
    "    for folder in result_folders:\n",
    "        config_name = folder.split(\"/\")[-1][:-20]\n",
    "        print(\"Results for config: {}\".format(config_name))\n",
    "        folder_results = []\n",
    "        files = natsort.natsorted(os.listdir(folder))\n",
    "\n",
    "        filtered_files = []\n",
    "        for i in range(len(files)):\n",
    "            # Ensures we don't load files passed by accident\n",
    "            if \".csv\" in files[i]:\n",
    "                filtered_files.append(\"{}/{}\".format(folder, files[i]))\n",
    "\n",
    "        i = 0\n",
    "        while i < len(filtered_files):\n",
    "            if len(filtered_files) < num_processes:\n",
    "                num_processes = len(filtered_files)\n",
    "            pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "            folder_results.append(pool.starmap(generate_results, zip(filtered_files[i: i + num_processes])))\n",
    "\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            i += num_processes\n",
    "\n",
    "        folder_results = [y for x in folder_results for y in x]\n",
    "        # Go through each of the available stats and write them out to a csv file.\n",
    "        output_csv_dir = \"/Users/brianmccarthy/git_repos/results-analysis/data/processed_data/{}/{}-{}\".format(\n",
    "            os.getcwd(), experiment_type,config_name, now)\n",
    "        \n",
    "        return folder_results\n",
    "\n",
    "        os.makedirs(output_csv_dir, exist_ok=True)\n",
    "\n",
    "        # Shortcut ensures we get the stats from the parsed results\n",
    "        for stat in folder_results[0]:\n",
    "            if stat == \"CBR\":\n",
    "                across_run_results_cbr(folder_results, output_csv_dir)\n",
    "            else:\n",
    "                across_run_results(folder_results, stat, output_csv_dir, \"Distance\")\n",
    "\n",
    "        processed_results.append(output_csv_dir)\n",
    "\n",
    "    print(\"Folders processed: {}\".format(processed_results))\n",
    "    return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_results(output_csv):\n",
    "\n",
    "    print(\"Generating results for file: {}\".format(output_csv))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    pdr_sci_agg = pd.DataFrame()\n",
    "    pdr_tb_agg = pd.DataFrame()\n",
    "    # pdr_tb_ignore_sci_agg = pd.DataFrame()\n",
    "    ipg_agg = pd.DataFrame()\n",
    "    cbr_agg = pd.DataFrame()\n",
    "    unsensed_errors = pd.DataFrame()\n",
    "    hd_errors = pd.DataFrame()\n",
    "    prop_errors = pd.DataFrame()\n",
    "    interference_errors = pd.DataFrame()\n",
    "\n",
    "    error_dfs = {}\n",
    "    # Need a new for loop through all the errors and adding them as a stat distance\n",
    "    for error in results_[\"errors\"]:\n",
    "        error_dfs[error] = pd.DataFrame()\n",
    "    \n",
    "    for chunk in pd.read_csv(output_csv, chunksize=10 ** 6):\n",
    "        \n",
    "        # CBR calculation doesn't aggregate the same way as the above so dealt with separately\n",
    "        cbr_df = chunk[[\"Time\", \"cbr\"]]\n",
    "        cbr_df = cbr_df[cbr_df[\"cbr\"] > -1]\n",
    "\n",
    "        if cbr_agg.empty:\n",
    "            cbr_agg = cbr_df\n",
    "        else:\n",
    "            cbr_agg = cbr_agg.append(cbr_df)\n",
    "            \n",
    "\n",
    "        # SCI PDR calculation\n",
    "        pdr_sci_agg = stat_distance(pdr_sci_agg, chunk, \"sciDecoded\", \"txRxDistanceSCI\", True)\n",
    "\n",
    "        # TB PDR calculation\n",
    "        pdr_tb_agg = stat_distance(pdr_tb_agg, chunk, \"tbDecoded\", \"txRxDistanceTB\", True)\n",
    "\n",
    "        # pdr_tb_ignore_sci_agg = self.stat_distance(pdr_tb_agg, chunk, \"tbDecodedIgnoreSCI\", \"txRxDistanceTB\", True)\n",
    "\n",
    "        # IPG calculation\n",
    "        ipg_agg = stat_distance(ipg_agg, chunk, \"interPacketDelay\", \"txRxDistanceTB\", False)\n",
    "\n",
    "        chunk = chunk[chunk[\"tbReceived\"] != -1]\n",
    "        for error in error_dfs:\n",
    "            if \"sci\" in error[0:3]:\n",
    "                error_dfs[error] = stat_distance(error_dfs[error], chunk, error, \"txRxDistanceSCI\", True)\n",
    "            else:\n",
    "                error_dfs[error] = stat_distance(error_dfs[error], chunk, error, \"txRxDistanceTB\", True)\n",
    "\n",
    "    results[\"PDR-SCI\"] = pdr_sci_agg\n",
    "    results[\"PDR-TB\"] = pdr_tb_agg\n",
    "    # results[\"PDR-IGNORE-SCI\"] = pdr_tb_ignore_sci_agg\n",
    "    results[\"IPG\"] = ipg_agg\n",
    "    results[\"CBR\"] = cbr_agg\n",
    "\n",
    "    for key, df in zip([\"unsensed_errors\", \"hd_errors\", \"prop_errors\", \"interference_errors\"],\n",
    "                       [unsensed_errors, hd_errors, prop_errors, interference_errors]):\n",
    "        for error in results_[key]:\n",
    "            if df.empty:\n",
    "                df = error_dfs[error]\n",
    "            else:\n",
    "                # Combine mean errors\n",
    "                df[\"mean\"] = df[\"mean\"] + error_dfs[error][\"mean\"]\n",
    "\n",
    "        results[key] = df\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_distance(agg_df, df, stat, distance, percentage):\n",
    "\n",
    "    # Reduce the size of the DF to what we're interested in.\n",
    "    distance_df = df[df[stat].notnull()]\n",
    "    distance_df = distance_df[(distance_df[\"posX\"] > 0) & (distance_df[\"posX\"] < 2000)]\n",
    "    distance_df = distance_df[[\"Time\", \"NodeID\", stat, distance]]\n",
    "    distance_df = distance_df[distance_df[stat] > -1]\n",
    "    distance_df = distance_df.rename(columns={\"Time\": \"Time\", \"NodeID\": \"NodeID\", stat: stat, distance: \"Distance\"})\n",
    "\n",
    "    # Only interested in max 500m simply as it's not all that relevant to go further.\n",
    "    # Note that going to the max distance of the file can cause issues with how they are parsed.\n",
    "    max_distance = min(530, distance_df[\"Distance\"].max())\n",
    "\n",
    "    # Get the mean, std, count for each distance\n",
    "    distance_df = distance_df.groupby(\n",
    "        pd.cut(distance_df[\"Distance\"], np.arange(0, max_distance, 10))).agg(\n",
    "        {stat: [np.mean, \"count\"]})\n",
    "\n",
    "    # Remove over head column\n",
    "    distance_df.columns = distance_df.columns.droplevel()\n",
    "\n",
    "    if percentage:\n",
    "        distance_df = distance_df.apply(lambda x: x * 100, axis=1)\n",
    "\n",
    "    if agg_df.empty:\n",
    "        agg_df = distance_df\n",
    "    else:\n",
    "        # combine_chunks\n",
    "        agg_df = pd.merge(agg_df, distance_df, on=\"Distance\", how='outer')\n",
    "        agg_df = agg_df.apply(combine_line, axis=1, result_type='expand')\n",
    "        agg_df = agg_df.rename({0: \"mean\", 1: \"count\"}, axis='columns')\n",
    "\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_line(line):\n",
    "    mean_a = line[\"mean_x\"]\n",
    "    count_a = line[\"count_x\"]\n",
    "\n",
    "    mean_b = line[\"mean_y\"]\n",
    "    count_b = line[\"count_y\"]\n",
    "\n",
    "    if np.isnan(mean_a) and np.isnan(mean_b):\n",
    "        return [mean_a, count_a]\n",
    "    elif np.isnan(mean_a) and not np.isnan(mean_b):\n",
    "        return [mean_b, count_b]\n",
    "    elif np.isnan(mean_b) and not np.isnan(mean_a):\n",
    "        return [mean_a, count_a]\n",
    "    else:\n",
    "        ex_a = mean_a * count_a\n",
    "        ex_b = mean_b * count_b\n",
    "\n",
    "        tx = ex_a + ex_b\n",
    "        tn = count_a + count_b\n",
    "\n",
    "        overall_mean = tx / tn\n",
    "        overall_count = tn\n",
    "\n",
    "        return [overall_mean, overall_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def across_run_results(results, stat, output_csv_dir, merge_col):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    print(\"Statistic of interest: {}\".format(stat))\n",
    "    for i in range(len(results)):\n",
    "        if df.empty:\n",
    "            df = results[i][stat]\n",
    "        else:\n",
    "            df = pd.merge(df, results[i][stat], how='outer', on=merge_col,\n",
    "                          suffixes=(i, i + 1),\n",
    "                          copy=True, indicator=False)\n",
    "\n",
    "    mean_cols = df.filter(regex='mean').columns\n",
    "\n",
    "    n = len(mean_cols) - 1\n",
    "    t_value = t.ppf(p, n)\n",
    "\n",
    "    df = df.apply(combine_runs, axis=1, result_type='expand', args=(mean_cols, t_value,))\n",
    "    df = df.rename({0: \"Mean\", 1: \"Confidence-Interval\"}, axis='columns')\n",
    "    df.to_csv(\"{}/{}.csv\".format(output_csv_dir, stat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def across_run_results_cbr(results, output_csv_dir):\n",
    "    earliest_time = float(\"inf\")\n",
    "    latest_time = -float(\"inf\")\n",
    "\n",
    "    raw_cbr_df = pd.DataFrame()\n",
    "    for folder in results:\n",
    "\n",
    "        start_time = folder[\"CBR\"][\"Time\"].min()\n",
    "        if start_time < earliest_time:\n",
    "            earliest_time = start_time\n",
    "\n",
    "        end_time = folder[\"CBR\"][\"Time\"].max()\n",
    "        if end_time > latest_time:\n",
    "            latest_time = end_time\n",
    "\n",
    "        if raw_cbr_df.empty:\n",
    "            raw_cbr_df = folder[\"CBR\"]\n",
    "        else:\n",
    "            raw_cbr_df.append(folder[\"CBR\"])\n",
    "\n",
    "    print(\"Earliest time: {}s Latest time: {}s\".format(earliest_time, latest_time))\n",
    "\n",
    "    cbr_df = pd.DataFrame(columns=[\"Mean\", \"Time\", \"Confidence-Interval\"])\n",
    "    last_time = earliest_time\n",
    "    for i in np.arange(earliest_time, latest_time, 0.1):\n",
    "        subsection_df = pd.DataFrame()\n",
    "        for folder in results:\n",
    "            df = folder[\"CBR\"]\n",
    "            if subsection_df.empty:\n",
    "                subsection_df = df[(df[\"Time\"] < i) & (df[\"Time\"] >= last_time) & (df[\"cbr\"].notnull())]\n",
    "            else:\n",
    "                subsection_df.append(df[(df[\"Time\"] < i) & (df[\"Time\"] >= last_time) & (df[\"cbr\"].notnull())])\n",
    "\n",
    "        last_time = i\n",
    "\n",
    "        cbr_df = cbr_df.append({\"Mean\": subsection_df[\"cbr\"].mean(),\n",
    "                                \"Time\": (i + last_time) / 2,\n",
    "                                \"Confidence-Interval\": subsection_df[\"cbr\"].std()\n",
    "                                }, ignore_index=True)\n",
    "\n",
    "    cbr_df.to_csv(\"{}/CBR.csv\".format(output_csv_dir), index=False)\n",
    "    raw_cbr_df.to_csv(\"{}/raw-CBR.csv\".format(output_csv_dir), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_runs(line, mean_cols, t_value):\n",
    "    means = []\n",
    "    for mean in mean_cols:\n",
    "        means.append(line[mean])\n",
    "\n",
    "    n = len(means)\n",
    "\n",
    "    # Average Across runs\n",
    "    xBar = sum(means) / n\n",
    "\n",
    "    # Deviation between runs and average\n",
    "    deviation = []\n",
    "    for mean in means:\n",
    "        deviation.append((mean - xBar) ** 2)\n",
    "    s2 = sum(deviation) / (n - 1)\n",
    "\n",
    "    # Confidence interval\n",
    "    ci = t_value * math.sqrt(s2 / n)\n",
    "\n",
    "    return [xBar, ci]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_graph(folders, graph_type, graph_title, graph_info, now):\n",
    "    means = []\n",
    "    cis = []\n",
    "    distances = []\n",
    "    for folder in folders:\n",
    "        df = pd.read_csv(\"{}/{}.csv\".format(folder, graph_type))\n",
    "        means.append(list(df[\"Mean\"]))\n",
    "        if confidence_intervals:\n",
    "            cis.append(list(df[\"Confidence-Interval\"]))\n",
    "        distances = (list(range(0, df.shape[0] * 10, 10)))\n",
    "        \n",
    "    graph_info[\"means\"] = means\n",
    "    graph_info[\"cis\"] = cis\n",
    "\n",
    "    if graph_type in [\"PDR-SCI\", \"PDR-TB\"]:\n",
    "        dist_graph(distances, graph_info, \"{}-{}\".format(graph_title, graph_type),\n",
    "                   ylabel=\"Packet Delivery Rate %\", now=now, confidence_intervals=cis, show=False, store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbr_graph(folders, graph_type, graph_title, graph_info, now):\n",
    "    # Might change this to time based graph but CBR is fine for now\n",
    "    times = []\n",
    "    cbrs = []\n",
    "    cis = []\n",
    "    for folder in folders:\n",
    "        cbr = []\n",
    "        time = []\n",
    "        ci = []\n",
    "        df = pd.read_csv(\"{}/CBR.csv\".format(folder))\n",
    "        times.append(list(df[\"Time\"]))\n",
    "        cbrs.append(list(df[\"cbr\"]))\n",
    "        if confidence_intervals:\n",
    "            cis.append(list(df[\"Confidence-Interval\"]))\n",
    "        \n",
    "\n",
    "    graph_info[\"means\"] = cbrs\n",
    "    graph_info[\"times\"] = times\n",
    "    graph_info[\"cis\"] = cis\n",
    "\n",
    "    cbr_plot(graph_info, \"{}-{}\".format(graph_title, graph_type), now=now,\n",
    "             confidence_intervals=confidence_intervals, show=True, store=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_graph(distances, graph_info, plot_name, ylabel, now, legend_pos=\"lower left\",\n",
    "               confidence_intervals=None, show=True, store=False):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(len(graph_info[\"config_name\"])):\n",
    "        if confidence_intervals:\n",
    "            ax.errorbar(distances, means[i], yerr=confidence_intervals[i], label=labels[i])\n",
    "        else:\n",
    "            ax.plot(distances, graph_info[\"means\"][i], label=graph_info[\"labels\"][i],\n",
    "                    fillstyle=\"none\", marker=graph_info[\"markers\"][i], markevery=5,\n",
    "                    color=graph_info[\"colors\"][i], linestyle=graph_info[\"linestyles\"][i])\n",
    "\n",
    "    ax.set(xlabel='Distance (m)', ylabel=ylabel)\n",
    "    ax.legend(loc=legend_pos)\n",
    "    ax.tick_params(direction='in')\n",
    "    \n",
    "    ax.set_xlim([0, 500])\n",
    "    ax.set_ylim([0, 100])\n",
    "    plt.xticks(np.arange(0, (max(distances) + 1), step=50))\n",
    "    plt.yticks(np.arange(0, 101, step=10))\n",
    "    plt.grid(b=True, alpha=0.5)\n",
    "\n",
    "    if show:\n",
    "        fig.show()\n",
    "        \n",
    "    print(\"{}/{}-{}\".format(figure_store, plot_name, now))\n",
    "\n",
    "    if store:\n",
    "        fig.savefig(\"{}/{}-{}.png\".format(figure_store, plot_name, now), dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbr_plot(graph_info, plot_name, now, confidence_intervals=None, show=True, store=False):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i in range(len(graph_info[\"config_name\"])):\n",
    "        if confidence_intervals:\n",
    "            ax.errorbar(graph_info[\"times\"][i], graph_info[\"means\"][i], yerr=confidence_intervals[i],\n",
    "                        label=graph_info[\"labels\"][i],\n",
    "                        fillstyle=\"none\", color=graph_info[\"colors\"][i], linestyle=graph_info[\"linestyles\"][i])\n",
    "        else:\n",
    "            ax.plot(graph_info[\"times\"][i], graph_info[\"means\"][i], label=graph_info[\"labels\"][i],\n",
    "                    marker=graph_info[\"markers\"][i], markevery=5, fillstyle=\"none\",\n",
    "                    color=graph_info[\"colors\"][i], linestyle=graph_info[\"linestyles\"][i])\n",
    "\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set(xlabel='Time (s)', ylabel='Channel Busy Ratio %')\n",
    "    ax.tick_params(direction='in')\n",
    "\n",
    "    ax.set_ylim([0, 100])\n",
    "    plt.yticks(np.arange(0, 101, step=10))\n",
    "    plt.grid(b=True, alpha=0.5)\n",
    "\n",
    "    if show:\n",
    "        fig.show()\n",
    "\n",
    "    if store:\n",
    "        fig.savefig(\"{}/{}-{}.png\".format(figure_store, plot_name, now), dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_dist(distances, decoded, decoded_labels, errors, error_labels, plot_name):\n",
    "    # TODO: Update to allow such graphing to be automatically configured.\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    if use_markers:\n",
    "        for i in range(len(decoded)):\n",
    "            ax.plot(distances, decoded[i], label=decoded_labels[i], marker=markers[i], markevery=3)\n",
    "\n",
    "            for j in range(len(errors[i])):\n",
    "                ax.plot(distances, errors[i][j], label=error_labels[i][j], marker=markers[i + j])\n",
    "\n",
    "    elif use_line_types:\n",
    "        for i in range(len(decoded)):\n",
    "            ax.plot(distances, decoded[i], label=decoded_labels[i])\n",
    "\n",
    "            for j in range(len(errors[i])):\n",
    "                ax.plot(distances, errors[i][j], label=error_labels[i][j])\n",
    "\n",
    "    else:\n",
    "        for i in range(len(decoded)):\n",
    "            ax.plot(distances, decoded[i], label=decoded_labels[i])\n",
    "\n",
    "            for j in range(len(errors[i])):\n",
    "                ax.plot(distances, errors[i][j], label=error_labels[i][j])\n",
    "\n",
    "    ax.legend(loc='center left')\n",
    "\n",
    "    ax.set(xlabel='Distance (m)', ylabel='Packet Delivery Rate (PDR) %')\n",
    "    ax.grid()\n",
    "\n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.yticks(np.arange(0, 1.1, step=.1))\n",
    "\n",
    "    ax.set_xlim([0, (max(distances) + 1)])\n",
    "    plt.xticks(np.arange(0, (max(distances) + 1), step=50))\n",
    "\n",
    "    fig.savefig(\"{}/{}-{}.png\".format(figure_store, plot_name, now), dpi=300)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/Users/brianmccarthy/git_repos/results-analysis/configs/cv2x.json\"\n",
    "with open(config_file) as config_json:\n",
    "    config = json.load(config_json)[experiment_type]\n",
    "results_ = config[\"results\"]\n",
    "p = results_[\"confidence-interval\"]\n",
    "now=\"00_00_00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for config: Fast\n",
      "Generating results for file: /Users/brianmccarthy/git_repos/results-analysis/data/parsed_data/cv2x/Fast-2020-05-28-23_03_07/run-2.csv\n",
      "Generating results for file: /Users/brianmccarthy/git_repos/results-analysis/data/parsed_data/cv2x/Fast-2020-05-28-23_03_07/run-1.csv\n",
      "Generating results for file: /Users/brianmccarthy/git_repos/results-analysis/data/parsed_data/cv2x/Fast-2020-05-28-23_03_07/run-4.csv\n",
      "Generating results for file: /Users/brianmccarthy/git_repos/results-analysis/data/parsed_data/cv2x/Fast-2020-05-28-23_03_07/run-3.csv\n",
      "Generating results for file: /Users/brianmccarthy/git_repos/results-analysis/data/parsed_data/cv2x/Fast-2020-05-28-23_03_07/run-5.csv\n",
      "Min Time: 500.004s Max Time: 507.588s in this chunk\n",
      "Min Time: 500.005s Max Time: 507.628s in this chunk\n",
      "Min Time: 500.004s Max Time: 507.557s in this chunk\n",
      "Min Time: 500.006s Max Time: 507.585s in this chunk\n",
      "Min Time: 500.001s Max Time: 507.524s in this chunk\n",
      "Min Time: 507.59s Max Time: 511.999s in this chunk\n",
      "Min Time: 507.63s Max Time: 512.0s in this chunk\n",
      "Min Time: 507.525s Max Time: 512.0s in this chunk\n",
      "Min Time: 507.557s Max Time: 512.0s in this chunk\n",
      "Min Time: 507.586s Max Time: 512.0s in this chunk\n"
     ]
    }
   ],
   "source": [
    "folder_results = prepare_results([\"/Users/brianmccarthy/git_repos/results-analysis/data/parsed_data/cv2x/Fast-2020-05-28-23_03_07\"], now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time    500.006\n",
      "cbr       0.000\n",
      "dtype: float64\n",
      "Time    507.585000\n",
      "cbr       0.537415\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(folder_results[0][\"CBR\"].min())\n",
    "print(folder_results[0][\"CBR\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
