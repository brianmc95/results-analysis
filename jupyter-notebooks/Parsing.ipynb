{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from itertools import repeat\n",
    "from natsort import natsorted\n",
    "import shutil\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dec_line_pattern = re.compile(\"^vector\")\n",
    "vector_res_line_pattern = re.compile(\"^\\d+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_of_interest = [\n",
    "    \"sciSent\",\n",
    "    \"tbSent\",\n",
    "    \"txRxDistanceSCI\",\n",
    "    \"txRxDistanceTB\",\n",
    "    \"sciReceived\",\n",
    "    \"sciDecoded\",\n",
    "    \"sciFailedHalfDuplex\",\n",
    "    \"tbReceived\",\n",
    "    \"tbDecoded\",\n",
    "    \"tbFailedDueToNoSCI\",\n",
    "    \"tbFailedHalfDuplex\",\n",
    "    \"tbFailedButSCIReceived\",\n",
    "    \"sciUnsensed\",\n",
    "    \"missedTransmission\",\n",
    "    \"generatedGrants\",\n",
    "    \"selectedSubchannelIndex\",\n",
    "    \"selectedNumSubchannels\",\n",
    "    \"grantBreak\",\n",
    "    \"grantBreakMissedTrans\",\n",
    "    \"grantBreakSize\",\n",
    "    \"tbFailedDueToProp\",\n",
    "    \"tbFailedDueToInterference\",\n",
    "    \"sciFailedDueToProp\",\n",
    "    \"sciFailedDueToInterference\",\n",
    "    \"cbr\",\n",
    "    \"subchannelReceived\",\n",
    "    \"subchannelsUsed\",\n",
    "    \"senderID\",\n",
    "    \"subchannelsUsedToSend\",\n",
    "    \"subchannelSent\",\n",
    "    \"grantStartTime\",\n",
    "    \"selectedSubchannelIndex\",\n",
    "    \"selectedNumSubchannels\",\n",
    "    \"posX\",\n",
    "    \"posY\",\n",
    "    \"interPacketDelay\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_data(real_vector_path, json_fields, output_csv, read=True, pivot=True, combine=True):\n",
    "    # Simply remove the :vector part of vector names from both sets of vectors.\n",
    "    found_vector = False\n",
    "    for field in json_fields:\n",
    "        if \":vector\" in field:\n",
    "            found_vector = True\n",
    "            break\n",
    "\n",
    "    if found_vector:\n",
    "        json_fields = remove_vectors(json_fields)\n",
    "\n",
    "    print(json_fields)\n",
    "\n",
    "    print(\"Beginning parsing of vector file: {}\".format(real_vector_path))\n",
    "\n",
    "    # Read the vector file into a csv file\n",
    "    chunk_folder = output_csv.split(\".\")[0]\n",
    "    \n",
    "    if read:\n",
    "        read_vector_file(output_csv, real_vector_path, json_fields)\n",
    "\n",
    "    print(\"File read, begin pivoting csv file: {}\".format(real_vector_path))\n",
    "    \n",
    "    if pivot:\n",
    "        csv_pivot(chunk_folder, json_fields)\n",
    "\n",
    "    print(\"Pivot complete, consolidate chunk files for {}\".format(output_csv))\n",
    "    \n",
    "    if combine:\n",
    "        combine_files(chunk_folder, output_csv)\n",
    "\n",
    "    print(\"Finished parsing of vector file: {}\".format(real_vector_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_pivot(directory, stats):\n",
    "    orig_loc = os.getcwd()\n",
    "    os.chdir(directory)\n",
    "\n",
    "    csv_files = os.listdir(os.getcwd())\n",
    "    csv_files = natsorted(csv_files)\n",
    "    header = True\n",
    "    for csv_file in csv_files:\n",
    "        if \".csv\" in csv_file:\n",
    "            print(\"Pivoting chunk file: {}\".format(csv_file))\n",
    "            chunk_df = pd.read_csv(csv_file)\n",
    "\n",
    "            chunk_df = chunk_df.infer_objects()\n",
    "\n",
    "            chunk_df = chunk_df.sort_values(by=[\"NodeID\", \"Time\"])\n",
    "            # Parse the vector file to ensure it is formatted correclty.\n",
    "            chunk_df['seq'] = chunk_df.groupby([\"Time\", \"NodeID\", \"StatisticName\"]).cumcount()\n",
    "\n",
    "            chunk_df = chunk_df.pivot_table(\"Value\", [\"Time\", \"NodeID\", \"seq\"], \"StatisticName\")\n",
    "            chunk_df.reset_index(inplace=True)\n",
    "            chunk_df = chunk_df.drop([\"seq\"], axis=1)\n",
    "\n",
    "            # Ensure all fields correctly filled\n",
    "            for field in stats:\n",
    "                if field not in chunk_df.columns:\n",
    "                    chunk_df[field] = np.nan\n",
    "\n",
    "            # Ensure the order of the files is also correct\n",
    "            chunk_df = chunk_df.reindex(sorted(chunk_df.columns), axis=1)\n",
    "\n",
    "            chunk_df.to_csv(csv_file, index=False, header=header)\n",
    "            header = False\n",
    "\n",
    "            del chunk_df\n",
    "\n",
    "    os.chdir(orig_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(csv_directory, outfile):\n",
    "    destination = open(outfile, 'wb')\n",
    "\n",
    "    orig_loc = os.getcwd()\n",
    "    os.chdir(csv_directory)\n",
    "\n",
    "    csv_files = os.listdir(os.getcwd())\n",
    "    csv_files = natsorted(csv_files)\n",
    "    for csv_file in csv_files:\n",
    "        if \".csv\" in csv_file and csv_file != outfile:\n",
    "            print(\"Merging chunk file: {} into {}\".format(csv_file, outfile))\n",
    "            shutil.copyfileobj(open(csv_file, 'rb'), destination)\n",
    "            os.remove(csv_file)\n",
    "    destination.close()\n",
    "\n",
    "    os.chdir(orig_loc)\n",
    "\n",
    "    os.rmdir(csv_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup \n",
    "real_vector_path = \"/hdd/results-analysis/data/omnet/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.vec\"\n",
    "output_csv =\"/hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sciSent', 'tbSent', 'txRxDistanceSCI', 'txRxDistanceTB', 'sciReceived', 'sciDecoded', 'sciFailedHalfDuplex', 'tbReceived', 'tbDecoded', 'tbFailedDueToNoSCI', 'tbFailedHalfDuplex', 'tbFailedButSCIReceived', 'sciUnsensed', 'missedTransmission', 'generatedGrants', 'selectedSubchannelIndex', 'selectedNumSubchannels', 'grantBreak', 'grantBreakMissedTrans', 'grantBreakSize', 'tbFailedDueToProp', 'tbFailedDueToInterference', 'sciFailedDueToProp', 'sciFailedDueToInterference', 'cbr', 'subchannelReceived', 'subchannelsUsed', 'senderID', 'subchannelsUsedToSend', 'subchannelSent', 'grantStartTime', 'selectedSubchannelIndex', 'selectedNumSubchannels', 'posX', 'posY', 'interPacketDelay']\n",
      "Beginning parsing of vector file: /hdd/results-analysis/data/omnet/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.vec\n",
      "File read, begin pivoting csv file: /hdd/results-analysis/data/omnet/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.vec\n",
      "Pivoting chunk file: chunk-0.csv\n",
      "Pivoting chunk file: chunk-1.csv\n",
      "Pivoting chunk file: chunk-2.csv\n",
      "Pivoting chunk file: chunk-3.csv\n",
      "Pivoting chunk file: chunk-4.csv\n",
      "Pivoting chunk file: chunk-5.csv\n",
      "Pivoting chunk file: chunk-6.csv\n",
      "Pivoting chunk file: chunk-7.csv\n",
      "Pivoting chunk file: chunk-8.csv\n",
      "Pivoting chunk file: chunk-9.csv\n",
      "Pivoting chunk file: chunk-10.csv\n",
      "Pivoting chunk file: chunk-11.csv\n",
      "Pivoting chunk file: chunk-12.csv\n",
      "Pivoting chunk file: chunk-13.csv\n",
      "Pivoting chunk file: chunk-14.csv\n",
      "Pivot complete, consolidate chunk files for /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-0.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-1.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-2.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-3.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-4.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-5.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-6.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-7.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-8.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-9.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-10.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-11.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-12.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-13.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Merging chunk file: chunk-14.csv into /hdd/results-analysis/data/parsed_data/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.csv\n",
      "Finished parsing of vector file: /hdd/results-analysis/data/omnet/cv2x/RRI-Adaptation-CR-Limit-50-2020-05-28-10_01_47/run-1.vec\n"
     ]
    }
   ],
   "source": [
    "tidy_data(real_vector_path, stats_of_interest, output_csv, read=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
